[[server]]
name = "kmd0"
[server.config]
address = "https://host.docker.internal:8120"
enabled = true
passkey = "g1ImlB3yyCCcLct6Lt2Ffx5969oVpCGR"

##

[[server]]
name = "kmd1"
[server.config]
address = "https://10.10.10.36:8120"
enabled = true
passkey = "k9sFdHxaIyKy00rel7gyxDdGkN4C8Ft8"

##

[[stack]]
name = "1pconnect"
[stack.config]
server = "kmd1"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  op-connect-api:
    image: 1password/connect-api:latest
    env_file: .env
    ports:
      - "8082:8080"
    volumes:
      - ${CDATA}/1password-credentials.json:/home/opuser/.op/1password-credentials.json
      - "data:/home/opuser/.op/data"
    restart: unless-stopped
  op-connect-sync:
    image: 1password/connect-sync:latest
    env_file: .env
    ports:
      - "8083:8080"
    volumes:
      - ${CDATA}/1password-credentials.json:/home/opuser/.op/1password-credentials.json
      - "data:/home/opuser/.op/data"
    restart: always

volumes:
  data:
"""
environment = """
CDATA=/mnt/cFS/appdata/1pconnect
"""

##

[[stack]]
name = "authentik"
[stack.config]
server = "kmd1"
links = ["https://auth.d3adc3ii.cc"]
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  postgresql:
    env_file:
    - .env
    environment:
      POSTGRES_DB: ${PG_DB:-authentik}
      POSTGRES_PASSWORD: ${PG_PASS:?database password required}
      POSTGRES_USER: ${PG_USER:-authentik}
    healthcheck:
      interval: 30s
      retries: 5
      start_period: 20s
      test:
      - CMD-SHELL
      - pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}
      timeout: 5s
    image: docker.io/library/postgres:16-alpine
    restart: unless-stopped
    volumes:
    - database:/var/lib/postgresql/data
  redis:
    command: --save 60 1 --loglevel warning
    healthcheck:
      interval: 30s
      retries: 5
      start_period: 20s
      test:
      - CMD-SHELL
      - redis-cli ping | grep PONG
      timeout: 3s
    image: docker.io/library/redis:alpine
    restart: unless-stopped
    volumes:
    - redis:/data
  server:
    command: server
    depends_on:
      postgresql:
        condition: service_healthy
      redis:
        condition: service_healthy
    env_file:
    - .env
    environment:
      AUTHENTIK_POSTGRESQL__HOST: postgresql
      AUTHENTIK_POSTGRESQL__NAME: ${PG_DB:-authentik}
      AUTHENTIK_POSTGRESQL__PASSWORD: ${PG_PASS}
      AUTHENTIK_POSTGRESQL__USER: ${PG_USER:-authentik}
      AUTHENTIK_REDIS__HOST: redis
      AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY:?secret key required}
    image: ${AUTHENTIK_IMAGE:-ghcr.io/goauthentik/server}:${AUTHENTIK_TAG:-2025.8.4}
    ports:
    - ${COMPOSE_PORT_HTTP:-9000}:9000
    - ${COMPOSE_PORT_HTTPS:-9443}:9443
    restart: unless-stopped
    volumes:
    - ${CAPP}/media:/media
    - ${CAPP}/custom-templates:/templates
  worker:
    command: worker
    depends_on:
      postgresql:
        condition: service_healthy
      redis:
        condition: service_healthy
    env_file:
    - .env
    environment:
      AUTHENTIK_POSTGRESQL__HOST: postgresql
      AUTHENTIK_POSTGRESQL__NAME: ${PG_DB:-authentik}
      AUTHENTIK_POSTGRESQL__PASSWORD: ${PG_PASS}
      AUTHENTIK_POSTGRESQL__USER: ${PG_USER:-authentik}
      AUTHENTIK_REDIS__HOST: redis
      AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY:?secret key required}
    image: ${AUTHENTIK_IMAGE:-ghcr.io/goauthentik/server}:${AUTHENTIK_TAG:-2025.8.4}
    restart: unless-stopped
    user: root
    volumes:
    - /var/run/docker.sock:/var/run/docker.sock
    - ${CAPP}/media:/media
    - ${CAPP}/certs:/certs
    - ${CAPP}/custom-templates:/templates
volumes:
  database:
    driver: local
  redis:
    driver: local
"""
environment = """
CAPP=/mnt/cFS/appdata/authentik
PG_PASS=[[AUTHENTIK_PG_PASS]]
AUTHENTIK_SECRET_KEY=[[AUTHENTIK_SECRET_KEY]]
AUTHENTIK_ERROR_REPORTING__ENABLED=true
"""

##

[[stack]]
name = "domotz"
[stack.config]
server = "kmd0"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  domotz-collector:
    image: "domotz/domotz-collector"
    network_mode: "host"
    container_name: domotz-collector
    volumes:
      - /mnt/cFS/appdata/domotz/config/domotz.json:/opt/domotz/etc/domotz.json
    cap_add:
      - NET_ADMIN
    restart: unless-stopped
"""

##

[[stack]]
name = "homepage"
[stack.config]
server = "kmd0"
links = ["https://homepage.d3adc3ii.cc"]
poll_for_updates = true
auto_update = true
auto_update_all_services = true
destroy_before_deploy = true
linked_repo = "dIIIhl"
run_directory = "komodo/homepage"
file_paths = ["compose.yaml"]
config_files = [
  { path = "config/services.yaml" },
  { path = "config/settings.yaml" },
  { path = "config/docker.yaml" },
  { path = "config/bookmarks.yaml" }
]
environment = """
HOMEPAGE_DATA_DIR= /mnt/cFS/appdata/komodo/repos/dIIIhl/komodo/homepage
#HOMEPAGE_ALLOWED_HOSTS= "homepage.d3adc3ii.cc,homepage.d3adc3ii.site,10.10.10.35:3731,homeppage.d3adc3ii.cc"
HOMEPAGE_ALLOWED_HOSTS=*
PGID= 1000
PUID= 1000 
# Site Config
HOMEPAGE_VAR_TITLE= "d3 Homepage"
HOMEPAGE_VAR_FAVICON= "/icons/d3logo.png"
HOMEPAGE_VAR_IMG_URL= "/images/hack.jpg"
HOMEPAGE_VAR_HEADER_STYLE= boxed #boxedWidgets

HOMEPAGE_VAR_IMG_BLUR= md
HOMEPAGE_VAR_IMG_SATURATE= 50
HOMEPAGE_VAR_IMG_BRIGHTNESS= 50
HOMEPAGE_VAR_IMG_OPACITY=70
HOMEPAGE_VAR_USE_EQUAL_HEIGHTS= true
HOMEAGE_VAR_DISABLE_COLLAPSE= true


### GLANCES WIDGET SETTINGS
HOMEPAGE_VAR_GL11_URL= http://10.10.10.11:61208
HOMEPAGE_VAR_GL11_VERSION= 4 # required only if running glances v4 or higher, defaults to 3
HOMEPAGE_VAR_GL11_CPU= false # optional, enabled by default, disable by setting to false
HOMEPAGE_VAR_GL11_MEM= false # optional, enabled by default, disable by setting to false
HOMEPAGE_VAR_GL11_CPUTEMP= true # disabled by default
HOMEPAGE_VAR_GL11_UPTIME= true # disabled by default
HOMEPAGE_VAR_GL11_LABEL= pve11 # optional

HOMEPAGE_VAR_GL12_URL= http://10.10.10.12:61208
HOMEPAGE_VAR_GL12_VERSION= 4 
HOMEPAGE_VAR_GL12_CPU= false 
HOMEPAGE_VAR_GL12_MEM= false 
HOMEPAGE_VAR_GL12_CPUTEMP= true 
HOMEPAGE_VAR_GL12_UPTIME= true 
HOMEPAGE_VAR_GL12_LABEL= pve12 

HOMEPAGE_VAR_GL13_URL= http://10.10.10.13:61208
HOMEPAGE_VAR_GL13_VERSION= 4
HOMEPAGE_VAR_GL13_CPU= false 
HOMEPAGE_VAR_GL13_MEM= false 
HOMEPAGE_VAR_GL13_CPUTEMP= true 
HOMEPAGE_VAR_GL13_UPTIME= true 
HOMEPAGE_VAR_GL13_LABEL= pve13 

#### INFRA ####################################################################################################################################################################################################
# Proxmox
HOMEPAGE_VAR_PROXMOX_ICON= "/icons/proxmox-light.png"
HOMEPAGE_VAR_PROXMOX_URL_PVE11= "https://pve.d3adc3ii.cc"
HOMEPAGE_VAR_PROXMOX_IP_PVE11="https://10.10.10.10:8006"
#HOMEPAGE_VAR_PROXMOX_USER=homepage@pve!homepage 
HOMEPAGE_VAR_PROXMOX_SECRET=bd9d76fb-00a6-4e4d-af8a-66f0606ba7fc
HOMEPAGE_VAR_PBS_URL= "https://pbs.d3adc3ii.site"
# Truenas
HOMEPAGE_VAR_TRUENAS_ICON= "/icons/truenas-scale-light.png"
HOMEPAGE_VAR_TRUENAS_URL= "https://truenas.d3adc3ii.cc"
HOMEPAGE_VAR_TRUENAS_KEY=4-wEIiKMeFJpOJPyS2Y2658EQGbxZkBJUcQyBQvQoRzjggfnOIXNN7z8w7OvKpgK6C
HOMEPAGE_VAR_TRUENAS_ENABLEDPOOL= true
# Komodo
HOMEPAGE_VAR_KOMODO_ICON= "/icons/docker-light.png"
HOMEPAGE_VAR_KOMODO_URL= "https://komodo.d3adc3ii.cc"
HOMEPAGE_VAR_KOMODO_IPURL= "http://10.10.10.25:9120"
HOMEPAGE_VAR_KOMODO_KEY=K-69zvNUYoxn5KREW5nVpLqeLa741U7NHmPXoEx4S1
HOMEPAGE_VAR_KOMODO_SECRET=S-5hhHrLWPfLHPJCRGBMV2auU7QsqfUaGrTJYuOW9M

# OMNI
HOMEPAGE_VAR_OMNI_ICON= "/icons/omni.png"
HOMEPAGE_VAR_OMNI_URL= "https://omni.d3hl.site"


#### DOMAIN ####################################################################################################################################################################################################
# Authentik
HOMEPAGE_VAR_AUTHENTIK_ICON= "/icons/authentik-light.png"
HOMEPAGE_VAR_AUTHENTIK_URL= "https://auth.d3adc3ii.cc"
HOMEPAGE_VAR_AUTHENTIK_KEY= "ZnhLrOvUoWZfHcQpFrteQVekOjpGQh48lXEVFn34HvGtT8eJoyhgaNYWb2mI"
# DNS
HOMEPAGE_VAR_DNS_ICON= "/icons/technitium-light.png"
HOMEPAGE_VAR_DNS_URL= "https://ns.d3adc3ii.site"
HOMEPAGE_VAR_DNS_KEY= [[HOMEPAGE_VAR_DNS_KEY]]
HOMEPAGE_VAR_DNS_RANGE: LastDay # optional, defaults to LastHour
# GoDoxy
HOMEPAGE_VAR_GODOXY_ICON= "/icons/godoxy-light.png"
HOMEPAGE_VAR_GODOXY_URL= "https://doxy.d3hl.site"
# PANGOLIN
HOMEPAGE_VAR_PANGOLIN_ICON= "/icons/pangolin-light.png"
HOMEPAGE_VAR_PANGOLIN_URL= "https://pangolin.d3adc3ii.cc"

#### NETWORK ####################################################################################################################################################################################################
# NetAlertX 
HOMEPAGE_VAR_NETALERTX_ICON= "/icons/netalertx-light.png"
HOMEPAGE_VAR_NETALERTX_URL= "https://netalertx.d3adc3ii.site"
HOMEPAGE_VAR_NETALERTX_KEY="t_pJ26L2qEeUcR6bz5RCGI"
HOMEPAGE_VAR_NETALERTX_FIELDS= ["connected","down_alerts","new_devices"] 
# UPTIME KUMA
HOMEPAGE_VAR_KUMA_ICON= "/icons/uptime-kuma-light.png"
HOMEPAGE_VAR_KUMA_URL= "https://uptime.d3adc3ii.cc"
HOMEPAGE_VAR_KUMA_KEY=uk1_CJtVJXDT8DFll97KXiPPXb6dEWHpOdBZXymKWLPn
# PHPIPAM
HOMEPAGE_VAR_PHPIPAM_URL= "https://phpipam.d3adc3ii.cc/"
HOMEPAGE_VAR_PHPIPAM_ICON= "sh-phpmyadmin-light"
# Nautobot
HOMEPAGE_VAR_NAUTOBOT_URL= "https://nautobot.d3adc3ii.cc/"
HOMEPAGE_VAR_NAUTOBOT_ICON= "/icons/netbox-light.png"
# NETBOX CLOUD
HOMEPAGE_VAR_NETBOX_ICON= "/icons/netbox-light.png"
HOMEPAGE_VAR_NETBOXCL_URL= "https://wmfk3018.cloud.netboxapp.com"
# NETBOX CONSOLE
HOMEPAGE_VAR_NETBOXCS_URL= "https://console.netboxlabs.com/"# NETBOX CONSOLE
HOMEPAGE_VAR_NETBOXCS_URL= "https://console.netboxlabs.com/"

#### MONITORING  ####################################################################################################################################################################################################
# Beszel
HOMEPAGE_VAR_BESZEL_ICON= "/icons/beszel-light.png"
HOMEPAGE_VAR_BESZEL_URL= "https://beszel.d3adc3ii.cc"
HOMEPAGE_VAR_BESZEL_USERNAME= "d3tech@pm.me"
HOMEPAGE_VAR_BESZEL_PASSWORD= "KVG6qpu0rnb-ybd9etn" 
HOMEPAGE_VAR_BESZEL_VERSION= "2"
HOMEPAGE_VAR_BESZEL_SYSTEMID_pve11= pve11
HOMEPAGE_VAR_BESZEL_SYSTEMID_pve12= pve12
HOMEPAGE_VAR_BESZEL_SYSTEMID_pve13= pve13
HOMEPAGE_VAR_BESZEL_SYSTEMID_pbs= pbs
HOMEPAGE_VAR_BESZEL_FIELDS= ["cpu","memory","disk","network"]
# SPEEDTEST
HOMEPAGE_VAR_SPEEDTEST_ICON= "sh-speedtest-tracker-light"
HOMEPAGE_VAR_SPEEDTEST_URL= "https://speedtest.d3adc3ii.site"
# Wazuh
HOMEPAGE_VAR_WAZUH_ICON= "/icons/wazuh-light.png"
HOMEPAGE_VAR_WAZUH_URL= "https://wazuh.d3adc3ii.site"
# APPRISE-API
HOMEPAGE_VAR_APPRISE_ICON= "/icons/gotify-dark.png"
HOMEPAGE_VAR_APPRISE_URL= "http://192.168.2.33:8000"
# Healthchecks
HOMEPAGE_VAR_HEALTHCHECKS_ICON= "/icons/healthchecks-light.png"
HOMEPAGE_VAR_HEALTHCHECKS_URL= "http://health.d3adc3ii.site"

#### BACKUP ####################################################################################################################################################################################################
# PBS
HOMEPAGE_VAR_PBS_URL="https://pbs.d3hl.site"
HOMEPAGE_VAR_PBS_USER=homepage@pbs!homepage
HOMEPAGE_VAR_PBS_SECRET=56ff3a13-0620-432b-b173-d048b1298a50
# Backrest
HOMEPAGE_VAR_BACKREST_ICON= "/icons/backrest-light.png"
HOMEPAGE_VAR_BACKREST_URL= "https://backrest.d3adc3ii.site"

# Remote-Backups
HOMEPAGE_VAR_REMOTEBACKUP_ICON= "/icons/proxmox-light.png"
HOMEPAGE_VAR_REMOTEBACKUP_URL= "https://dashboard.remote-backups.com"


#### MEDIA ####################################################################################################################################################################################################
# Jellyfin
HOMEPAGE_VAR_JELLY_ICON= "/icons/jellyfin-light.png"
HOMEPAGE_VAR_JELLY_URL= "https://jelly.d3adc3ii.site"
HOMEAGE_VAR_JELLY_WIDGETURL= "http://10.10.10.27:8096"
HOMEPAGE_VAR_JELLY_KEY=a6a45c61dd1d4f059b02fa22ad8c0ef3
HOMEPAGE_VAR_JELLY_ENABLEBLOCK=true
HOMEPAGE_VAR_JELLY_NOWPLAY=false

# Immich
HOMEPAGE_VAR_IMMICH_ICON = "/icons/immich-light.png"
HOMEPAGE_VAR_IMMICH_URL= "https://immich.d3hl.site"
HOMEPAGE_VAR_IMMICH_KEY=W7IJnjVEIhc7d72AAm4sxJ49YJI1kdtkiVvmCZipVqc
HOMEPAGE_VAR_IMMICH_VERSION= 2
HOMEPAGE_VAR_IMMICH_FIELDS= ["storage","photos","videos"] 
# Stash
HOMEPAGE_VAR_STASH_ICON= "/icons/stash-light.png"
HOMEPAGE_VAR_STASH_URL= "https://stash.d3adc3ii.site"
HOMEPAGE_VAR_STASH_USER= "d3adc3ii"
HOMEPAGE_VAR_QBIT_API= "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1aWQiOiJkMyIsInN1YiI6IkFQSUtleSIsImlhdCI6MTc0NjAzOTExOX0.OQqokQMNgdCRNxfPOQGOTFH4GCB0pQvZbIS9orkgkrg"

#### ACCESS ####################################################################################################################################################################################################
# Guacamole
HOMEPAGE_VAR_GUACAMOLE_ICON= "/icons/apache-guacamole-light.png"
HOMEPAGE_VAR_GUACAMOLE_URL= "https://guaca.d3adc3ii.site/guacamole"
HOMEPAGE_VAR_GUACAMOLEEXT_URL= "https://guaca.d3adc3ii.cc/guacamole"
# SHELLHUB
HOMEPAGE_VAR_SHELLHUB_ICON= "sh-shellhub-light"
HOMEPAGE_VAR_SHELLHUB_URL= "http://192.168.2.33"

#### LOGGING ####################################################################################################################################################################################################
# Grafana
HOMEPAGE_VAR_GRAFANA_ICON= "/icons/grafana-light.png"
HOMEPAGE_VAR_GRAFANA_URL= "https://d3adc3ii.grafana.net/dashboards"
HOMEPAGE_VAR_GRAFANA_URL= "https://grafana.d3adc3ii.cc"
HOMEPAGE_VAR_GRAFANAC_URL= "https://grafana.d3adc3ii.cc/d/n5bu_kv45/traefik-official-standalone-dashboard"


HOMEPAGE_VAR_INFLUX_ICON= "sh-influx-light"
HOMEPAGE_VAR_INFLUX_URL= "https://influxdb.d3adc3ii.site"
# Scutiny
HOMEPAGE_VAR_SCRUTINY_ICON= "/icons/scrutiny-light.png"
HOMEPAGE_VAR_SCRUTINY_URL= "https://scrutiny.d3adc3ii.cc" 
# Dozzle
HOMEPAGE_VAR_DOZZLE_ICON= "sh-dozzle-light"
HOMEPAGE_VAR_DOZZLE_URL= "https://dozzle.d3adc3ii.site"
# Pulse
HOMEPAGE_VAR_PULSEXT_ICON= "sh-proxmox-light"
HOMEPAGE_VAR_PULSEXT_URL= "https://pulse.d3adc3ii.cc"

#### APPS ####################################################################################################################################################################################################
# Actual Budget
HOMEPAGE_VAR_ACTUAL_ICON= "sh-actual-budget-light"
HOMEPAGE_VAR_ACTUAL_URL= "https://actual.d3adc3ii.site"
# Sensei
HOMEPAGE_VAR_SENSEI_ICON= "sh-sentry-light"
HOMEPAGE_VAR_SENSEI_URL= "https://statement.d3adc3ii.site"
# Semaphore
HOMEPAGE_VAR_SEMAPHORE_ICON= "/icons/semaphore-ui-light.png"
HOMEPAGE_VAR_SEMAPHORE_URL= "https://semaphore.d3adc3ii.site"
# Aria2
HOMEPAGE_VAR_ARIA_ICON= "sh-aria2-light"
HOMEPAGE_VAR_ARIA_URL= "https://dl.d3adc3ii.site"
# NZBGET
HOMEPAGE_VAR_NZBGET_ICON= "sh-nzbget-light"
HOMEPAGE_VAR_NZBGET_URL= "https://nzbget.d3adc3ii.site"
# Karakeep
HOMEPAGE_VAR_KARAKEEP_ICON= "/icons/karakeep.png"
HOMEPAGE_VAR_KARAKEEP_URL= "https://kara.d3adc3ii.cc"
# Wallos
HOMEPAGE_VAR_WALLOS_ICON= "/icons/wallos-light.png"
HOMEPAGE_VAR_WALLOS_URL= "https://wallos.d3adc3ii.cc"

#### TOOLS ####################################################################################################################################################################################################

#### EXTRAS ####################################################################################################################################################################################################
# Selfhst-icons
HOMEPAGE_VAR_SELFHST_ICON= "sh-selfh-st-light" 
HOMEPAGE_VAR_SELFHST_URL= "https://selfhst-icons.d3adc3ii.cc"

###################   CORP    ##################### 

# Komodo
HOMEPAGE_VAR_KOMODOCORP_URL="http://10.203.1.121:9120"
# Fenrus
HOMEPAGE_VAR_FENRUS_ICON="sh-fenrus"
HOMEPAGE_VAR_FENRUS_URL="http://10.203.1.120:3222"
# PVECORP
HOMEPAGE_VAR_PVECORP_URL="https://10.203.1.113:8006"
# UNIMUS  
HOMEPAGE_VAR_UNIMUS_ICON="sh-unimus-light"
HOMEPAGE_VAR_UNIMUS_URL="http://10.203.1.120:8085"

### GL1 ###
# PVE11
HOMEPAGE_VAR_PVE11_URL= "https://pve11.d3adc3ii.cc"
HOMEPAGE_VAR_PVE12_URL= "10.10.10.12"
HOMEPAGE_VAR_PVE13_URL= "10.10.10.13"
HOMEPAGE_VAR_GLANCES_PVE11= "http://10.10.10.11:61208"
HOMEPAGE_VAR_GLANCES_PVE12= "http://10.10.10.12:61208"
HOMEPAGE_VAR_GLANCES_PVE13= "http://10.10.10.13:61208"
"""

##

[[stack]]
name = "immich-app"
[stack.config]
server = "kmd1"
file_contents = """
#
# WARNING: To install Immich, follow our guide: https://docs.immich.app/install/docker-compose
#
# Make sure to use the docker-compose.yml of the current release:
#
# https://github.com/immich-app/immich/releases/latest/download/docker-compose.yml
#
# The compose file on main may not be compatible with the latest release.

name: immich

services:
  immich-server:
    container_name: immich_server
    image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release}
    # extends:
    #   file: hwaccel.transcoding.yml
    #   service: cpu # set to one of [nvenc, quicksync, rkmpp, vaapi, vaapi-wsl] for accelerated transcoding
    volumes:
      # Do not edit the next line. If you want to change the media storage location on your system, edit the value of UPLOAD_LOCATION in the .env file
      - ${UPLOAD_LOCATION}:/data
      - /etc/localtime:/etc/localtime:ro
    env_file:
      - .env
    ports:
      - '2283:2283'
    depends_on:
      - redis
      - database
    restart: always
    healthcheck:
      disable: false

  immich-machine-learning:
    container_name: immich_machine_learning
    # For hardware acceleration, add one of -[armnn, cuda, rocm, openvino, rknn] to the image tag.
    # Example tag: ${IMMICH_VERSION:-release}-cuda
    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION:-release}
    # extends: # uncomment this section for hardware acceleration - see https://docs.immich.app/features/ml-hardware-acceleration
    #   file: hwaccel.ml.yml
    #   service: cpu # set to one of [armnn, cuda, rocm, openvino, openvino-wsl, rknn] for accelerated inference - use the `-wsl` version for WSL2 where applicable
    volumes:
      - model-cache:/cache
    env_file:
      - .env
    restart: always
    healthcheck:
      disable: false

  redis:
    container_name: immich_redis
    image: docker.io/valkey/valkey:8@sha256:81db6d39e1bba3b3ff32bd3a1b19a6d69690f94a3954ec131277b9a26b95b3aa
    healthcheck:
      test: redis-cli ping || exit 1
    restart: always

  database:
    container_name: immich_postgres
    image: ghcr.io/immich-app/postgres:14-vectorchord0.4.3-pgvectors0.2.0@sha256:bcf63357191b76a916ae5eb93464d65c07511da41e3bf7a8416db519b40b1c23
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_USER: ${DB_USERNAME}
      POSTGRES_DB: ${DB_DATABASE_NAME}
      POSTGRES_INITDB_ARGS: '--data-checksums'
      # Uncomment the DB_STORAGE_TYPE: 'HDD' var if your database isn't stored on SSDs
      # DB_STORAGE_TYPE: 'HDD'
    volumes:
      # Do not edit the next line. If you want to change the database storage location on your system, edit the value of DB_DATA_LOCATION in the .env file
      - ${DB_DATA_LOCATION}:/var/lib/postgresql/data
    shm_size: 128mb
    restart: always

volumes:
  model-cache:
"""
environment = """
IMMICH_IGNORE_MOUNT_CHECK_ERRORS=true
# You can find documentation for all the supported env variables at https://docs.immich.app/install/environment-variables
PGID=1000
PUID=1000
# The location where your uploaded files are stored
UPLOAD_LOCATION=/mnt/zApps/imm_library

# The location where your database files are stored. Network shares are not supported for the database
DB_DATA_LOCATION=./postgres

# To set a timezone, uncomment the next line and change Etc/UTC to a TZ identifier from this list: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones#List
# TZ=Etc/UTC

# The Immich version to use. You can pin this to a specific version like "v2.1.0"
IMMICH_VERSION=v2.3.1

# Connection secret for postgres. You should change it to a random password
# Please use only the characters `A-Za-z0-9`, without special characters or spaces
DB_PASSWORD=postgres2taolavodch

# The values below this line do not need to be changed
###################################################################################
DB_USERNAME=postgres
DB_DATABASE_NAME=immich
"""

##

[[stack]]
name = "kestra"
[stack.config]
server = "kmd1"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
volumes:
  postgres-data:
    driver: local
  kestra-data:
    driver: local

services:
  postgres:
    image: postgres:18
    volumes:
      - postgres-data:/var/lib/postgresql
    env_file: .env
    environment:
      POSTGRES_DB: kestra
      POSTGRES_USER: kestra
      POSTGRES_PASSWORD: k3str4
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      interval: 30s
      timeout: 10s
      retries: 10
    restart: unless-stopped
  kestra:
    image: kestra/kestra:latest
    pull_policy: always
    # Kestra, by default, has a termination grace period of 5m. We need to wait a little more to be sure no active tasks are running.
    stop_grace_period: 6m
    # Note that this setup with a root user is intended for development purpose.
    # Our base image runs without root, but the Docker Compose implementation needs root to access the Docker socket
    user: "root"
    command: server standalone
    volumes:
      - kestra-data:/app/storage
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/kestra-wd:/tmp/kestra-wd
    env_file: .env
    restart: unless-stopped
    environment:
      KESTRA_CONFIGURATION: |
        datasources:
          postgres:
            url: jdbc:postgresql://postgres:5432/kestra
            driverClassName: org.postgresql.Driver
            username: kestra
            password: k3str4
        kestra:
          # server:
          #   basic-auth:
          #     username: admin@kestra.io # it must be a valid email address
          #     password: Admin1234 # it must be at least 8 characters long with uppercase letter and a number
          repository:
            type: postgres
          storage:
            type: local
            local:
              base-path: "/app/storage"
          queue:
            type: postgres
          tasks:
            tmp-dir:
              path: /tmp/kestra-wd/tmp
          url: http://localhost:8080/
    ports:
      - "8080:8080"
      - "8081:8081"
    depends_on:
      postgres:
        condition: service_started
"""
environment = """
GH_USERNAME=[[GH_USERNAME]]
GH_ACCESS_TOKEN=[[GH_ACCESS_TOKEN]]
"""

##

[[stack]]
name = "komodo-ee"
[stack.config]
server = "kmd0"
file_contents = """
---

services:
  ansible:
    image: ghcr.io/bpbradley/ansible/komodo-ee:v1.3 # or latest
    extra_hosts:
      - host.docker.internal:host-gateway
    volumes:
      - ./ansible:/ansible # Mount ansible files into container
      - /mnt/cFS/appdata/komodo/komodo_ansible:/root/.ssh/id_ed25519:ro # Make sure the user you run the container has read access to the key
    env_file:
      - .env
    environment:
      ANSIBLE_HOST_KEY_CHECKING: ${ANSIBLE_HOST_KEY_CHECKING:-false} # Necessary for automation, unless you manage known_hosts and map it into container
    command: "sleep 3600" # this keeps the container running by default, which will help with testing so you can exec into it temporarily
"""
environment = """
PUID=1000
PGID=1000
"""

##

[[stack]]
name = "newt"
[stack.config]
server = "kmd0"
project_name = "pangolin"
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  newt:
    image: fosrl/newt
    container_name: newt
    restart: unless-stopped
    networks:
      - default
      - internal_web
    environment:
      - PANGOLIN_ENDPOINT=https://pangolin.d3adc3ii.cc
      - NEWT_ID=3fvxj21qpm1ipsv
      - NEWT_SECRET=fthakowv4ueg8ssvgqybrm1p6xj9h8rz35ceakf65taxhynt
      - DOCKER_SOCKET=tcp://socket-proxy:2375
     # - DOCKER_HOST=tcp://socket-proxy:2375


  socket-proxy:
    image: lscr.io/linuxserver/socket-proxy:latest
    container_name: socket-proxy
    networks:
      - internal_web
    environment:
      - ALLOW_START=1 #optional
      - ALLOW_STOP=1 #optional
      - ALLOW_RESTARTS=1 #optional
      - AUTH=1 #optional
      - BUILD=1 #optional
      - COMMIT=1 #optional
      - CONFIGS=1 #optional
      - CONTAINERS=1 #optional
      - DISABLE_IPV6=1 #optional
      - DISTRIBUTION=1 #optional
      - EVENTS=1 #optional
      - EXEC=1 #optional
      - IMAGES=1 #optional
      - INFO=1 #optional
      - LOG_LEVEL=info #optional
      - NETWORKS=1 #optional
      - NODES=1 #optional
      - PING=1 #optional
      - PLUGINS=1 #optional
      - POST=1 #optional
      - SECRETS=1 #optional
      - SERVICES=1 #optional
      - SESSION=1 #optional
      - SWARM=1 #optional
      - SYSTEM=1 #optional
      - TASKS=1 #optional
      - TZ=Etc/UTC #optional
      - VERSION=1 #optional
      - VOLUMES=1 #optional
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /run
networks:
  default:
  internal_web:
    external: true
"""
environment = """
  # VARIABLE = value
"""

##

[[stack]]
name = "patchmon"
[stack.config]
server = "kmd1"
links = ["https://patchmon.d3adc3ii.cc"]
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
# Change 3 Passwords in this file:
# Generate passwords with 'openssl rand -hex 64'
# 
# 1. The database password in the environment variable POSTGRES_PASSWORD
# 2. The redis password in the command redis-server --requirepass your-redis-password-here
# 3. The jwt secret in the environment variable JWT_SECRET
#
#
# Change 2 URL areas in this file:
# 1. Setup your CORS_ORIGIN to what url you will use for accessing PatchMon frontend url
# 2. Setup your SERVER_PROTOCOL, SERVER_HOST and SERVER_PORT to what you will use for linux agents to access PatchMon
#
# This is generally the same as your CORS_ORIGIN url , in some cases it might be different - SERVER_* variables are used in the scripts for Server connection.
# You can also change this in the front-end but in the case of docker-compose - it is overwritten by the variables set here.


name: patchmon

services:
  database:
    image: postgres:17-alpine
    restart: unless-stopped
    environment:
      POSTGRES_DB: patchmon_db
      POSTGRES_USER: patchmon_user
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U patchmon_user -d patchmon_db"]
      interval: 3s
      timeout: 5s
      retries: 7

  redis:
    image: redis:7-alpine
    restart: unless-stopped
    command: redis-server --requirepass nec9vhn2kht7davUGV # CHANGE THIS TO YOUR REDIS PASSWORD
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "--no-auth-warning", "-a", "nec9vhn2kht7dav*UGV", "ping"] # CHANGE THIS TO YOUR REDIS PASSWORD
      interval: 3s
      timeout: 5s
      retries: 7

  backend:
    image: ghcr.io/patchmon/patchmon-backend:latest
    restart: unless-stopped
    # See PatchMon Docker README for additional environment variables and configuration instructions
    environment:
      LOG_LEVEL: info
      DATABASE_URL: postgresql://patchmon_user:${POSTGRES_PASSWORD}@database:5432/patchmon_db
      JWT_SECRET: ${JWT_SECRET}
      SERVER_PROTOCOL: http
      SERVER_HOST: localhost
      SERVER_PORT: 3000
      CORS_ORIGIN: ${CORS_ORIGIN}
      # Database Connection Pool Configuration (Prisma)
      DB_CONNECTION_LIMIT: 30
      DB_POOL_TIMEOUT: 20
      DB_CONNECT_TIMEOUT: 10
      DB_IDLE_TIMEOUT: 300
      DB_MAX_LIFETIME: 1800
      # Rate Limiting (times in milliseconds)
      RATE_LIMIT_WINDOW_MS: 900000
      RATE_LIMIT_MAX: 5000
      AUTH_RATE_LIMIT_WINDOW_MS: 600000
      AUTH_RATE_LIMIT_MAX: 500
      AGENT_RATE_LIMIT_WINDOW_MS: 60000
      AGENT_RATE_LIMIT_MAX: 1000
      # Redis Configuration
      REDIS_HOST: redis
      REDIS_PORT: 6379
      REDIS_PASSWORD: nec9vhn2kht7davUGV
      REDIS_DB: 0
    volumes:
      - ${APP_DATA}/agents:/app/agents
    depends_on:
      database:
        condition: service_healthy
      redis:
        condition: service_healthy

  frontend:
    image: ghcr.io/patchmon/patchmon-frontend:latest
    restart: unless-stopped
    ports:
      - "3000:3000"
    depends_on:
      backend:
        condition: service_healthy

volumes:
  postgres_data:
  redis_data:
"""
environment = """
APP_DATA=/mnt/cFS/appdata/patchmon
CORS_ORIGIN= https://patchmon.d3adc3ii.cc
PGID=1000
PUID=1000
POSTGRES_DB=patchmon_db
POSTGRES_USER=patchmon_user
POSTGRES_PASSWORD=c3fdKLUcYnTmHkpaHNDGiWNGUjofsJZ
DATABASE_URL=postgresql://patchmon_user:c3fdKLUcYnTmHkpaHNDGiWNGUjofsJZ@vrc@database:5432/patchmon_db
REDIS_PASSWORD=nec9vhn2kht7davUGV
JWT_SECRET=e595840adb6788343dbf0f778f483c4f78be1af65484fc5f290cc4165e7d595a99fcafb8f012ed8fc720dddeb54662086ec644b11f52093e2bb64bc4212088b9
"""

##

[[stack]]
name = "phpipam"
[stack.config]
server = "kmd0"
file_contents = """
services:
  phpipam-web:
    image: phpipam/phpipam-www:latest
    networks:
      - default
      - internal_web
    ports:
      - "80:80"
    env_file: .env
    restart: always
    volumes:
      - ${APPDATA}/logo:/phpipam/css/images/logo
      - ${APPDATA}/ca:/usr/local/share/ca-certificates:ro
    depends_on:
      - phpipam-mariadb
    labels:
      - pangolin.public-resources.phpipam.name=phpipam
      - pangolin.public-resources.phpipam.full-domain=phpipam.d3adc3ii.cc
      - pangolin.public-resources.phpipam.protocol=http
      - pangolin.public-resources.phpipam.auth=sso-enabled
      - pangolin.public-resources.phpipam.auth.sso-roles=Admin
      - pangolin.public-resources.phpipam.healthcheck=true
      - pangolin.public-resources.phpipamh.healthcheck.hostname=phpipam
      - pangolin.public-resources.phpipamh.healthcheck.port=80

  phpipam-cron:
    image: phpipam/phpipam-cron:latest
    env_file: .env
    restart: always
    volumes:
      - ${APPDATA}/ca:/usr/local/share/ca-certificates:ro
    depends_on:
      - phpipam-mariadb

  phpipam-mariadb:
    image: mariadb:latest
    env_file: .env
    restart: always
    volumes:
      - ${APPDATA}/db:/var/lib/mysql

networks:
  internal_web:
    external: true
"""
environment = """
TZ=Asia/Singapore
IPAM_DATABASE_HOST=phpipam-mariadb
IPAM_DATABASE_PASS=[[IPAM_DATABASE_PASS]]
IPAM_DATABASE_WEBHOST=%
SCAN_INTERVAL=1h
APPDATA=/mnt/cFS/appdata/phpipam
MYSQL_USER: 'd3'
MYSQL_ROOT_PASSWORD=[[IPAM_DATABASE_PASS]]
"""

##

[[stack]]
name = "pulse"
[stack.config]
server = "kmd1"
file_contents = """
services:
  pulse:
    image: rcourtman/pulse:latest
    container_name: pulse
    restart: unless-stopped
    ports:
      - "7655:7655"
    volumes:
      - $[DATA}:/data
      - /var/run/docker.sock:/var/run/docker.sock # Optional: Monitor local Docker
    env_file: .env
"""
environment = """
DATA=/mnt/cFS/appdata/pulse
PULSE_AUTH_USER=d3
PULSE_AUTH_PASS=abcd
"""

##

[[stack]]
name = "scanopy"
[stack.config]
server = "kmd1"
file_contents = """
name: scanopy

x-scanopy-env: &scanopy-env
  SCANOPY_LOG_LEVEL: ${SCANOPY_LOG_LEVEL:-info}
  SCANOPY_SERVER_PORT: ${SCANOPY_SERVER_PORT:-60072}
  SCANOPY_DAEMON_PORT: ${SCANOPY_DAEMON_PORT:-60073}

services:
  daemon:
    image: ghcr.io/scanopy/scanopy/daemon:latest
    container_name: scanopy-daemon
    network_mode: host
    privileged: true
    restart: unless-stopped
    ports:
      - "${SCANOPY_DAEMON_PORT:-60073}:${SCANOPY_DAEMON_PORT:-60073}"
    environment:
      <<: *scanopy-env
      SCANOPY_SERVER_URL: ${SCANOPY_SERVER_URL:-http://127.0.0.1:60072}
      SCANOPY_PORT: ${SCANOPY_DAEMON_PORT:-60073}
      SCANOPY_BIND_ADDRESS: ${SCANOPY_BIND_ADDRESS:-0.0.0.0}
      SCANOPY_NAME: ${SCANOPY_NAME:-scanopy-daemon}
      SCANOPY_HEARTBEAT_INTERVAL: ${SCANOPY_HEARTBEAT_INTERVAL:-30}
      SCANOPY_MODE: ${SCANOPY_MODE:-Push}
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:${SCANOPY_DAEMON_PORT:-60073}/api/health || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 15
    volumes:
      - daemon-config:/root/.config/daemon
      # Comment out the line below to disable docker discovery
      - /var/run/docker.sock:/var/run/docker.sock:ro

  postgres:
    image: postgres:17-alpine
    environment:
      POSTGRES_DB: scanopy
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped
    networks:
      - scanopy

  server:
    image: ghcr.io/scanopy/scanopy/server:latest
    ports:
      - "${SCANOPY_SERVER_PORT:-60072}:${SCANOPY_SERVER_PORT:-60072}"
    environment:
      <<: *scanopy-env
      SCANOPY_DATABASE_URL: postgresql://postgres:${POSTGRES_PASSWORD:-password}@postgres:5432/scanopy
      SCANOPY_WEB_EXTERNAL_PATH: /app/static
      SCANOPY_PUBLIC_URL: ${SCANOPY_PUBLIC_URL:-http://localhost:${SCANOPY_SERVER_PORT:-60072}}
      # How server reaches integrated daemon
      # 172.17.0.1 is Docker's default bridge gateway. If your's is different, make sure to change it.
      SCANOPY_INTEGRATED_DAEMON_URL: http://172.17.0.1:${SCANOPY_DAEMON_PORT:-60073}
    volumes:
      - ./data:/data
    depends_on:
      postgres:
        condition: service_healthy
      daemon:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - scanopy

volumes:
  postgres_data:
  daemon-config:

networks:
  scanopy:
    driver: bridge
    ipam:
      config:
        - subnet: 172.31.0.0/16
          gateway: 172.31.0.1
"""

##

[[stack]]
name = "twingate"
[stack.config]
server = "kmd1"
file_contents = """
services:
  twingate_connector:
    container_name: twingate
    image: twingate/connector:latest
    restart: always
    env_file:
    - .env
    sysctls:
      net.ipv4.ping_group_range: "0 65535"
"""
environment = """
TWINGATE_NETWORK=d3net
TWINGATE_ACCESS_TOKEN=eyJhbGciOiJFUzI1NiIsImtpZCI6Ii1NeDFKWVRyS3gzQjZ0T0NuMUFoX3J2OTdRNERzYksybUVuekZHczNnWG8iLCJ0eXAiOiJEQVQifQ.eyJhdWRzIjpudWxsLCJudCI6IkFOIiwiYWlkIjoiNjcxOTkxIiwiZGlkIjoiMjc1NTg0MyIsInJudyI6MTc2NDU3MTE1NSwianRpIjoiNDI4NDY3ZTgtODEwNy00ZmY3LThlNmQtNzgxNWI0ZGMxMmFiIiwiaXNzIjoidHdpbmdhdGUiLCJhdWQiOiJkM25ldCIsImV4cCI6MTc2NDU3NDUxMSwiaWF0IjoxNzY0NTcwOTExLCJ2ZXIiOiI0IiwidGlkIjoiMTAzNTY0Iiwicm5ldGlkIjoiMTM1OTMxIn0.CdZaPpe231AQ5pp2soFm5jpcRavScNLmv2YV4Xbn9zESR-f1FzUcB44hgoiLRm3zKzL-YJQR7XinfmYOx4PGmg
TWINGATE_REFRESH_TOKEN=Ecgu9VB-ikKlX9bAU1ntj-36WHVbIRul_mM2i3UxrkBitEPEWlUUO0xYUesxHLB01aGrl02FfeJ4KF7jW7haX4BifZiDdrf5B11GOkny69rfw05TVMXYCsgjb2yI9Zy9EFscQQ
TWINGATE_LABEL_HOSTNAME="`hostname`"
"""

##

[[repo]]
name = "dIIIhl"
[repo.config]
server = "kmd0"
git_provider = "Github.com"
git_account = "d3hl"
repo = "d3hl/dIIIhl"

##

[[procedure]]
name = "Backup Core Database"
description = "Triggers the Core database backup at the scheduled time."
tags = ["system"]
config.schedule = "Every day at 01:00"

[[procedure.config.stage]]
name = "Stage 1"
enabled = true
executions = [
  { execution.type = "BackupCoreDatabase", execution.params = {}, enabled = true }
]

##

[[procedure]]
name = "Global Auto Update"
description = "Pulls and auto updates Stacks and Deployments using 'poll_for_updates' or 'auto_update'."
tags = ["system"]
config.schedule = "Every day at 03:00"

[[procedure.config.stage]]
name = "Stage 1"
enabled = true
executions = [
  { execution.type = "GlobalAutoUpdate", execution.params = {}, enabled = true }
]

##

[[procedure]]
name = "pull repo + redeploy homepage"

[[procedure.config.stage]]
name = "Pull Repo"
enabled = true
executions = [
  { execution.type = "PullRepo", execution.params.repo = "dIIIhl", enabled = true }
]

[[procedure.config.stage]]
name = "Reploy Homepage"
enabled = true
executions = [
  { execution.type = "DeployStack", execution.params.stack = "homepage", execution.params.services = [], enabled = true }
]

##

[[action]]
name = "DeployPeriphery"
[action.config]
file_contents = """
type Server = { id: string; name: string; version: string; err?: Error };

function sleep(ms: number) { return new Promise(r => setTimeout(r, ms)); }
function parseContainerId(s: string): string | null { const m = s.match(/\b([0-9a-f]{12,64})\b/i); return m ? m[1] : null; }
function normalizeVersion(s: string | undefined | null): string { return String(s ?? "").trim().replace(/^v/i, ""); }

function truthy(v: unknown): boolean {
  if (typeof v === "boolean") return v;
  const s = String(v ?? "").trim().toLowerCase();
  return s === "true" || s === "1" || s === "yes" || s === "on";
}

function parseLimitServers(x: unknown): string[] {
  if (Array.isArray(x)) return x.map(String).map(s => s.trim()).filter(Boolean);
  const raw = String(x ?? "").trim();
  if (!raw) return [];
  try {
    const parsed = JSON.parse(raw);
    if (Array.isArray(parsed)) return parsed.map(String).map(s => s.trim()).filter(Boolean);
  } catch {}
  return raw.split(/[,\s]+/).map(s => s.trim()).filter(Boolean);
}

function extractRecap(s: string): string | null {
  const i = s.indexOf("PLAY RECAP");
  return i >= 0 ? s.slice(i) : null;
}

function recapHasFailures(recap: string): boolean {
  const failed = [...recap.matchAll(/failed=(\d+)/g)].some(([,n]) => parseInt(n,10) > 0);
  const unreachable = [...recap.matchAll(/unreachable=(\d+)/g)].some(([,n]) => parseInt(n,10) > 0);
  return failed || unreachable;
}

async function waitForServerUpdate(server: Server, timeoutMs = 40000, intervalMs = 1000): Promise<boolean> {
  const end = Date.now() + timeoutMs;
  while (Date.now() < end) {
    const { version } = (await komodo.read("GetPeripheryVersion", { server: server.id })) as Types.GetPeripheryVersionResponse;
    if (version === server.version) { console.log(`${server.name} Updated!`); return true; }
    console.debug(`Version: ${version}`)
    await sleep(intervalMs);
  }
  console.log(`${server.name} offline during update... Waiting to come back online...`);
  return false;
}

async function followContainerLogs(server: Server, containerId: string): Promise<string> {
  const term = `periphery-follow`;
  const streamCmd = `docker logs -f ${containerId}`;

  const getRecap = async (): Promise<string | null> => {
    let recapSeen = false;
    let recapText: string | null = null;
    try {
      await komodo.write("CreateTerminal", { 
        server: server.name, 
        name: term, 
        command: "/bin/bash", 
        recreate: Types.TerminalRecreateMode.Always 
      });
      await komodo.execute_terminal({ 
        server: server.name, 
        terminal: term, 
        command: `${streamCmd}` 
        },{
          onLine: (line) => {
            if (!recapSeen) {
              const i = line.indexOf("PLAY RECAP");
              if (i >= 0) {
                recapSeen = true; 
                const first = line.slice(i); 
                recapText = first;
                console.log(first);
              }
            } else {
              console.log(line);
              if (recapText) recapText += `\n${line}`;
            }
          },
          onFinish: () => {},
        }
      );
    } catch {}
    return recapSeen ? recapText : null;
  };

  const first = await getRecap();
  if (first) return first;
  // Server likely dropped out because it is currently updating. 
  // Wait a few seconds, then try to see if it comes back up, then try again
  await sleep(15000);
  const ok = await waitForServerUpdate(server);
  if (!ok) throw new Error(`Timeout waiting for ${server.name} to report version ${server.version}`);
  const second = await getRecap();
  if (!second) throw new Error(`No Ansible recap captured from ${server.name}`);
  return second;
}

async function resolveRequiredVersion(): Promise<string> {
  const req = String(ARGS.KOMODO_VERSION || "");
  if (req.toLowerCase() === "core") {
    const { version } = (await komodo.read("GetVersion", {})) as Types.GetVersionResponse;
    return normalizeVersion(version);
  }
  return normalizeVersion(req);
}

async function isServerOnline(id: string): Promise<boolean> {
  const res = await komodo.read("GetServerState", { server: id }) as Types.GetServerStateResponse;
  return res.status === Types.ServerState.Ok;
}

async function update() {
  const DRY_RUN = truthy(ARGS.DRY_RUN);
  const FORCE = truthy(ARGS.FORCE);
  const LIMIT_SERVERS = parseLimitServers(ARGS.LIMIT_SERVERS);
  const IGNORE_SERVERS = parseLimitServers(ARGS.IGNORE_SERVERS);

  const requiredVersion = await resolveRequiredVersion();
  if (!requiredVersion) throw new Error("Missing required version");

  const base = (await komodo.read("ListServers", { query: {} })) as Types.ListServersResponse;

  const allServers: Server[] = await Promise.all(
    base.map(async ({ id, name }) => {
      try {
        const { version } = (await komodo.read("GetPeripheryVersion", { server: id })) as Types.GetPeripheryVersionResponse;
        return { id, name, version };
      } catch (err) {
        return { id, name, version: "ERROR", err: err as Error };
      }
    })
  );

  const ignoreSet = new Set(IGNORE_SERVERS);
  const unknownIgnores = IGNORE_SERVERS.filter(
    v => !allServers.some(s => s.name === v || s.id === v)
  );
  let servers = allServers.filter(s => !ignoreSet.has(s.name) && !ignoreSet.has(s.id));

  if (IGNORE_SERVERS.length) {
    console.log(`Ignoring servers: ${IGNORE_SERVERS.join(", ") || "(none)"}`);
    if (unknownIgnores.length) console.log(`No match for ignored: ${unknownIgnores.join(", ")}`);
  }

  if (servers.length === 0) {
    console.log("ðŸ¦Ž All servers are ignored. Nothing to do. ðŸ¦Ž");
    return;
  }

  const byName = new Map(servers.map(s => [s.name, s]));
  const limits = LIMIT_SERVERS;
  const unknownLimits = limits.filter(n => !byName.has(n));
  if (limits.length) console.log(`Limiting to: ${limits.join(", ") || "(none)"}`);
  if (unknownLimits.length) console.log(`Ignoring unknown servers: ${unknownLimits.join(", ")}`);

  let candidates: Server[];
  if (limits.length) {
    candidates = limits.map(n => byName.get(n)).filter((x): x is Server => !!x);
    candidates = FORCE ? candidates : candidates.filter(s => !s.err && normalizeVersion(s.version) !== requiredVersion);
  } else if (FORCE) {
    candidates = servers.filter(s => !s.err);
  } else {
    candidates = servers.filter(s => !s.err && normalizeVersion(s.version) !== requiredVersion);
  }

  const targetIds = new Set(candidates.map(s => s.id));
  const labelWidth = Math.max(...servers.map(({ id, name }) => `${name} (id=${id})`.length));

  console.log("Periphery version check:");
  servers.forEach((s) => {
    const label = `${s.name} (id=${s.id})`.padEnd(labelWidth);
    const cur = normalizeVersion(s.version);
    const inScope = targetIds.has(s.id);

    let msg: string;
    if (s.err) {
      msg = `âŒ  Error: ${(s.err as Error).message}`;
    } else if (inScope) {
      if (FORCE && cur === requiredVersion) {
        msg = `ðŸ” forcing update (currently ${cur})`;
      } else if (cur !== requiredVersion) {
        msg = `ðŸŽ¯ target: ${cur} â†’ ${requiredVersion}`;
      } else {
        msg = `âœ… up to date${FORCE ? " (forcing update)" : ""}`;
      }
    } else {
      if (cur !== requiredVersion) {
        msg = `â­ï¸  not targeted (current ${cur}, required ${requiredVersion})`;
      } else {
        msg = `âœ… up to date`;
      }
    }

    console.log(`  - ${label} : ${msg}`);
  });

  if (candidates.length === 0) {
    console.log("ðŸ¦Ž Nothing to do. ðŸ¦Ž");
    return;
  }

  const stack = (await komodo.read("GetStack", { stack: ARGS.STACK_NAME })) as Types.GetStackResponse;
  const stackServerId = (stack as any).config?.server_id as string | undefined;
  const stackServer = servers.find(s => s.id === stackServerId);
  const includesStackServer = !!stackServer && candidates.some(s => s.id === stackServer.id);

  const DETACH = DRY_RUN ? false : includesStackServer;

  const allTargeted = candidates.length === servers.filter(s => !s.err).length;
  const allManaged = servers.filter(s => !s.err);
  let limitPattern: string | undefined;

  if (LIMIT_SERVERS.length || IGNORE_SERVERS.length) {
    limitPattern = candidates.map(s => s.name).join(",");
  } else {
    const allTargeted = candidates.length === allManaged.length;
    limitPattern = allTargeted ? undefined : candidates.map(s => s.name).join(",");
  }
  
  const command = [
    "ansible-playbook",
    ARGS.PLAYBOOK,
    "-i", ARGS.INVENTORY,
    "-e", `komodo_action=${ARGS.KOMODO_ACTION}`,
    "-e", `komodo_version=v${requiredVersion}`,
    "-e", "pause_after=true",
  ];
  if (limitPattern) command.push("-l", limitPattern);
  if (DRY_RUN) command.push("--check", "--diff");

  const result = (await komodo.execute_and_poll("RunStackService", {
    stack: ARGS.STACK_NAME,
    service: ARGS.SERVICE_NAME,
    command,
    detach: DETACH,
    pull: true,
    no_deps: true,
    env: { VAULT_PASS: "[[VAULT_PASS]]" },
  })) as Types.Update;

  const runLog = result.logs.find(l => l.stage === "Compose Run");
  if (!runLog) throw new Error("No 'Compose Run' stage found in logs.");

  let recapText: string | null = null;

  if (DETACH && stackServer) {
    // Detached: stdout/stderr should contain the container id; follow its logs to get the recap
    const cid = parseContainerId(`${runLog.stdout || ""}\n${runLog.stderr || ""}`);
    if (!cid) throw new Error("Could not parse container id from output; unable to follow logs.");

    console.log(`Following container logs (${cid}) on ${stackServer.name}â€¦`);

    // we expect this host to update to requiredVersion
    stackServer.version = requiredVersion;

    recapText = await followContainerLogs(stackServer, cid);
  } else {
    // Non-detached: compose output should include the recap
    if (!runLog.success) {
      console.error(runLog.stdout);
      console.error(runLog.stderr);
      throw new Error("Periphery Update Failed");
    }
    recapText = extractRecap(runLog.stdout) ?? extractRecap(runLog.stderr || "");
    if (!recapText) throw new Error("Ansible run completed but recap was not found in output.");
    console.log(recapText);
  }

  if (recapHasFailures(recapText)) throw new Error("Ansible recap indicates failures.");

  if (!DRY_RUN) {
    const offline = await Promise.all(
      candidates.map(async s => ({ s, ok: await isServerOnline(s.id) }))
    ).then(rows => rows.filter(r => !r.ok).map(r => r.s.name));
    if (offline.length) throw new Error(`Post-update check failed: offline servers: ${offline.join(", ")}`);
  }

  console.log("ðŸ¦Ž Periphery Update Successful ðŸ¦Ž");
}

await update();
"""
arguments_format = "json"
arguments = """
{
  "PLAYBOOK": "/ansible/playbooks/komodo.yml",
  "INVENTORY": "/ansible/inventory/komodo.yml",
  "KOMODO_VERSION": "core",
  "STACK_NAME": "komodo-ee",
  "SERVICE_NAME": "ansible",
  "DRY_RUN": false,
  "FORCE": false,
  "LIMIT_SERVERS": [],
  "IGNORE_SERVERS": []
}
"""

##

[[builder]]
name = "kmd0"
[builder.config]
type = "Server"
params = {}

##

[[resource_sync]]
name = "sync"
[resource_sync.config]
linked_repo = "dIIIhl"
resource_path = ["komodo/all.toml"]
managed = true