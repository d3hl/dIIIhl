[[server]]
name = "Public-70"
tags = ["10.10.10.70"]
[server.config]
address = "https://10.10.10.70:8120"
region = "Cantonment"
enabled = true

##

[[server]]
name = "k-120"
tags = ["10.203.1.120"]
[server.config]
address = "https://10.203.1.120:8120"
region = "Clementi"
enabled = true

##

[[server]]
name = "k-121"
tags = ["10.203.1.121"]
[server.config]
address = "https://10.203.1.121:8120"
region = "Clementi"
enabled = true

##

[[server]]
name = "komodo-1"
tags = ["10.10.10.25"]
[server.config]
address = "https://10.10.10.25:8120"
external_address = "https://komodo.d3adc3ii.cc"
region = "Cantonment"
enabled = true

##

[[server]]
name = "komodo-2"
tags = ["10.10.10.26"]
[server.config]
address = "https://10.10.10.27:8120"
region = "Cantonment"
enabled = true

##

[[server]]
name = "pangolin"
[server.config]
address = "https://45.127.32.141:8120"
region = "Orange"
enabled = true

##

[[stack]]
name = "actual-budget"
tags = [
  "internal",
  "production",
  "komodo-2"
]
[stack.config]
server = "komodo-2"
project_name = "actual-budget"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  actual_server:
    image: docker.io/actualbudget/actual-server:latest
    ports:
      - '5006:5006'
    env_file:
      - .env  
    volumes:
      - ${DOCKER_DATA}/actualbudget:/data
    healthcheck:
      # Enable health check for the instance
      test: ['CMD-SHELL', 'node src/scripts/health-check.js']
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: unless-stopped
    labels:
      proxy.aliases: actual
      proxy.actual.port: 5006
      proxy.idle_timeout: 30m
"""
environment = """
PUID=1000
PGID=1000
      # Uncomment any of the lines below to set configuration options.
      # - ACTUAL_HTTPS_KEY=/data/selfhost.key
      # - ACTUAL_HTTPS_CERT=/data/selfhost.crt
      # - ACTUAL_PORT=5006
      # - ACTUAL_UPLOAD_FILE_SYNC_SIZE_LIMIT_MB=20
      # - ACTUAL_UPLOAD_SYNC_ENCRYPTED_FILE_SYNC_SIZE_LIMIT_MB=50
      # - ACTUAL_UPLOAD_FILE_SIZE_LIMIT_MB=20
      # See all options and more details at https://actualbudget.github.io/docs/Installing/Configuration
      # !! If you are not using any of these options, remove the 'environment:' tag entirely.
"""

##


[[stack]]
name = "caddy"
tags = [
  "internal",
  "production",
  "core",
  "komodo-1"
]
[stack.config]
server = "Public-70"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  caddy: 
    name: caddy
    ipam:
      driver: default  
    
# Main Caddy
services:
  caddy:
    image: homeall/caddy-reverse-proxy-cloudflare:latest
    networks:
      - caddy
    ports:
      - 80:80
      - 443:443
      - "443:443/udp"
    env_file:
      - .env
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${DOCKER_DATA}/caddy/data:/data
      - ${DOCKER_DATA}/caddy/config/:/config/caddy  #ðŸ‘ˆ where to save configs 
    restart: unless-stopped
    extra_hosts:
      - host.docker.internal:host-gateway

# Caddy-config container
  caddy-config:
    container_name: caddy-config
    image: traefik/whoami:latest
    networks:
      - caddy
    restart: always
    env_file:
      - .env
    labels:
    #############################################
    # Settings and snippets to get things working
    # You shouldn't need to modify this normally
    # Custom settings and definitions are below
    #############################################

      #### Global Settings ####
      caddy_0.email: ${CADDY_EMAIL}
      caddy_0.auto_https: prefer_wildcard

      #### Snippets ####
      # Get wildcard certificate
      caddy_1: (wildcard)
      #caddy_1.acme_dns: "cloudflare ${CF_API_TOKEN}" 
      caddy_1.tls.dns: "cloudflare ${CF_API_TOKEN}"
      caddy_1.tls.resolvers: 1.1.1.1 1.0.0.1
      caddy_1.handle.abort: ""

      # Skip TLS verify for backend with self-signed HTTPS
      caddy_3: (https)
      caddy_3.transport: http
      caddy_3.transport.tls: ""
      caddy_3.transport.tls_insecure_skip_verify: ""

    ###########################################
    # Custom settings. Modify things below ðŸ‘‡:
    # Make sure they have unique label numbers
    ###########################################

      # Custom global settings, add/edit as needed
      # caddy_0.log: default
      # caddy_0.log.format: console

      # Uncomment this during testing to avoid hitting rate limit.
      # It will try to obtain SSL from Let's Encrypt's staging endpoint.
      # acme_ca: "https://acme-staging-v02.api.letsencrypt.org/directory" # ðŸ‘ˆ Staging

      ## Setup wildcard sites
      caddy_10: "*.d3adc3ii.site"   #ðŸ‘ˆ Change to your domain
      caddy_10.import: wildcard

      # Caddy Admin Endpoint and Metrics
      caddy_20: :2020
      #caddy_20.admin: "0.0.0.0:2019"
      #caddy_20.admin.origin: "caddy.d3adc3ii.site"
      caddy_20.handle: /reverse_proxy/upstreams
      caddy_20.handle.reverse_proxy: localhost:2019
      caddy_20.handle.reverse_proxy.header_up: Host localhost:2019
    

      # Add our first site, which this container itself
      caddy_99: whoami.d3adc3ii.site                       #ðŸ‘ˆ Subdomain using wildcard cert
      caddy_99.reverse_proxy: "{{upstreams 80}}"         #ðŸ‘ˆ Container port

      # For non-docker sites see https://gist.github.com/omltcat/241ef622070ca0580f2876a7cfa7de67
      # e.g.: Pi-Hole on another machine in the same LAN
      caddy_100: netalertx.d3adc3ii.site                      
      caddy_100.reverse_proxy: 192.168.2.31:20184

      caddy_101: pve.d3adc3ii.site                      
      caddy_101.reverse_proxy: 192.168.2.11:8006
      caddy_101.reverse_proxy.transport: http
      caddy_101.reverse_proxy.transport.tls_insecure_skip_verify: ""


      caddy_102: opn.d3adc3ii.site                      
      caddy_102.reverse_proxy: 192.168.2.1:2184
      caddy_102.reverse_proxy.transport: http
      caddy_102.reverse_proxy.transport.tls: ""
      caddy_102.reverse_proxy.transport.versions: 1.1 1.2
      caddy_102.reverse_proxy.transport.tls_insecure_skip_verify: ""

      caddy_103: truenas.d3adc3ii.site                      
      caddy_103.reverse_proxy: 192.168.2.7
      caddy_103.reverse_proxy.transport: http
      caddy_103.reverse_proxy.transport.tls_insecure_skip_verify: ""

      caddy_104: pbs.d3adc3ii.site   
      caddy_104.reverse_proxy: 192.168.2.35:8007
      caddy_104.reverse_proxy.transport: http
      caddy_104.reverse_proxy.transport.tls_insecure_skip_verify: ""

      caddy_105: ns0.d3adc3ii.site                      
      caddy_105.reverse_proxy: 192.168.2.5:5380
      caddy_106: stash.d3adc3ii.site                      
      caddy_106.reverse_proxy: 192.168.2.26:9999
      caddy_107: ha.d3adc3ii.site                      
      caddy_107.reverse_proxy: 192.168.99.5:8123    
      #caddy_108: jelly.d3adc3ii.site                      
      #caddy_108.reverse_proxy: 192.168.2.33:8096
      caddy_109: semaphore.d3adc3ii.site                      
      caddy_109.reverse_proxy: 192.168.2.28:3000            
      caddy_110: aria.d3adc3ii.site                      
      caddy_110.reverse_proxy: 192.168.2.16:6880  
      caddy_111: pulse.d3adc3ii.site                      
      caddy_111.reverse_proxy: 192.168.2.36:7655     
      caddy_112: backrest.d3adc3ii.site                      
      caddy_112.reverse_proxy: 192.168.2.35:9898
      caddy_113: actual.d3adc3ii.site                      
      caddy_113.reverse_proxy: 192.168.2.32:5006
      # wazuh
      caddy_114: wazuh.d3adc3ii.site                      
      caddy_114.reverse_proxy: 192.168.2.30:443
      caddy_114.reverse_proxy.transport: http
      caddy_114.reverse_proxy.transport.tls_insecure_skip_verify: ""
      # guaca
      caddy_115: guaca.d3adc3ii.site                      
      caddy_115.reverse_proxy: 192.168.2.20
      #caddy_116: checkmate.d3adc3ii.site                      
      #caddy_116.reverse_proxy: 192.168.2.33:52345  
      #caddy_117: checkmate2.d3adc3ii.site                      
      #caddy_117.reverse_proxy: 192.168.2.32:59232
      #caddy_118: checkmate3.d3adc3ii.site                      
      #caddy_118.reverse_proxy: 192.168.2.33:59232    

      # e.g. OpenMediaVault on the host machine, with self-signed https at port 4430
      #caddy_101: omv.example.com                         #ðŸ‘ˆ Subdomain using wildcard cert
      #caddy_101.reverse_proxy: host.docker.internal:4430 #ðŸ‘ˆ Port on host machine
      #caddy_101.reverse_proxy.import: https              #ðŸ‘ˆ Allow self-signed cert between OMV and Caddy
      #caddy_101.import: auth                             #ðŸ‘ˆ Enable protection by Authelia
"""
environment = """
PUID= [[PUID]]
PGID= [[PGID]]
CADDY_INGRESS_NETWORKS=caddy
CF_API_TOKEN= [[CF_API_TOKEN]]
CADDY_EMAIL= d3tech@pm.me
"""

##

[[stack]]
name = "discord-alerter"
[stack.config]
server = "komodo-2"
repo = "foxxmd/deploy-discord-alerter"
file_paths = ["compose.yaml"]
environment = """
  ## Required

  ## Your webhook URL
  DISCORD_WEBHOOK = [[DISCORD_WEBHOOK]]

  ## Optional

  ## Set whether to include Komodo Severity Level in notification title
  #LEVEL_IN_TITLE=true

  # Prefixes messages with a checkmark when the Alert is in the 'Resolved' state
  #INDICATE_RESOLVED=true

  # Filter if an alert is pushed based on its Resolved status
  # * leave unset to push all alerts
  # * otherwise, alerts will only be pushed if Alert is one of the comma-separated states set here
  #ALLOW_RESOLVED_TYPE=resolved,unresolved

  ## Delay alerts with below types for X milliseconds 
  ## and cancel pushing alert if it is resolved within that time
  #UNRESOLVED_TIMEOUT_TYPES=ServerCpu,ServerMem
  #UNRESOLVED_TIMEOUT=2000
"""

##

[[stack]]
name = "dockerproxy-26"
tags = ["komodo-2"]
[stack.config]
server = "komodo-2"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  d3internal:
    external: true
services:
  dockerproxy-26:
    image: lscr.io/linuxserver/socket-proxy:latest
    container_name: dockerproxy-26
    networks:
      - d3internal
    privileged: yes
    environment:
      - ALLOW_START=0 #optional
      - ALLOW_STOP=0 #optional
      - ALLOW_RESTARTS=0 #optional
      - AUTH=0 #optional
      - BUILD=0 #optional
      - COMMIT=0 #optional
      - CONFIGS=0 #optional
      - CONTAINERS=0 #optional
      - DISABLE_IPV6=0 #optional
      - DISTRIBUTION=0 #optional
      - EVENTS=1 #optional
      - EXEC=0 #optional
      - IMAGES=0 #optional
      - INFO=0 #optional
      - LOG_LEVEL=info #optional
      - NETWORKS=0 #optional
      - NODES=0 #optional
      - PING=1 #optional
      - PLUGINS=0 #optional
      - POST=0 #optional
      - SECRETS=0 #optional
      - SERVICES=0 #optional
      - SESSION=0 #optional
      - SWARM=0 #optional
      - SYSTEM=0 #optional
      - TASKS=0 #optional
      - VERSION=1 #optional
      - VOLUMES=0 #optional
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    restart: unless-stopped
    read_only: true
    tmpfs:
      - /run
"""

##


##

[[stack]]
name = "dozzle-agent-2"
tags = [
  "internal",
  "monitoring",
  "komodo-2"
]
[stack.config]
server = "komodo-2"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  dozzle-agent:
    image: amir20/dozzle:latest
    command: agent
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    ports:
      - 7007:7007
"""

##

##

[[stack]]
name = "dumbterm"
tags = [
  "external",
  "production",
  "komodo-2"
]
[stack.config]
server = "komodo-2"
links = ["https://dumbterm.d3adc3ii.cc"]
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  d3internal: 
    external: true
services:
  dumbterm:
    image: dumbwareio/dumbterm:latest
    container_name: dumbterm
    restart: unless-stopped
    networks:
      - default
      - d3internal
    ports:
      - ${DUMBTERM_PORT}:3000
    volumes:
      - ${DUMBTERM_CONFIG}:/root/.config
      - ${DUMBTERM_DATA_DIR}:/root/data
    environment:
      # Container timezone
      TZ: ${DUMBTERM_TZ}
      # The title shown in the web interface
      SITE_TITLE: ${DUMBTERM_SITE_TITLE:-DumbTerm}
      # Recommended PIN protection (leave empty to disable)
      DUMBTERM_PIN: ${DUMBTERM_PIN}
      # The base URL for the application
      BASE_URL: ${DUMBTERM_BASE_URL}
      ENABLE_STARSHIP: ${ENABLE_STARSHIP:-true}
      LOCKOUT_TIME: ${DUMBTERM_LOCKOUT_TIME:-15} # Minutes
      # Session duration in hours before requiring re-authentication
      MAX_SESSION_AGE: ${DUMBTERM_MAX_SESSION_AGE:-24} # Hours
      # (OPTIONAL) - List of allowed origins for CORS
      # ALLOWED_ORIGINS: ${DUMBTERM_ALLOWED_ORIGINS:-http://localhost:3000}
    labels:
      proxy.exclude: trure
      - homepage.group=Access
      - homepage.name=Dumbterm
      - homepage.icon=sh-dumbterm-light
      - homepage.href=https://dumbterm.d3adc3ii.cc
"""
environment = """
DUMBTERM_CONFIG="/mnt/zApps/dumbterm/config"
DUMBTERM_DATA_DIR="/mnt/zApps/dumbterm/data"
DUMBTERM_TZ="Asia/Singapore"
DUMBTERM_PIN=1111
DUMBTERM_PORT=3002
DUMBTERM_BASE_URL=http://dumbterm.d3adc3ii.cc:3002
"""

##


##

##

[[stack]]
name = "jellyfin"
tags = [
  "internal",
  "production",
  "komodo-3"
]
[stack.config]
server = "komodo-2"
project_name = "jellyfin"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  jellyfin:
    image: lscr.io/linuxserver/jellyfin:latest
    container_name: jelly
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Asia/Singapore
      - JELLYFIN_PublishedServerUrl=https://jelly.d3adc3ii.site #optional
    volumes:
      - ${DOCKER_DATA}/jellyfin/config:/config
      - ${JELLYFIN_DATA_DIR}/tvshows:/data/tvshows
      - ${JELLYFIN_DATA_DIR}/movies:/data/movies
    ports:
      - 8096:8096
      - 8920:8920 #optional
      - 7359:7359/udp #optional
      - 1900:1900/udp #optional
    restart: unless-stopped
    labels:
      proxy.jelly.port: 8096
"""
environment = """
JELLYFIN_DATA_DIR=/mnt/zFiles/media

"""

##

[[stack]]
name = "karakeep"
tags = [
  "external",
  "production",
  "komodo-2"
]
[stack.config]
server = "komodo-2"
links = ["https://kara.d3adc3ii.cc/"]
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  d3internal:
    external: true
services:
  web:
    image: ghcr.io/karakeep-app/karakeep:${KARAKEEP_VERSION:-release}
    restart: unless-stopped
    volumes:
      - ${DOCKER_DATA}/karakeep/data:/data
    networks:
      - default
      - d3internal
    ports:
      - 3333:3000
    env_file:
      - .env
    environment:
      MEILI_ADDR: http://meilisearch:7700
      BROWSER_WEB_URL: http://chrome:9222
      DATA_DIR: /data
    labels:
      - homepage.group=Applications
      - homepage.name=Karakeep
      - homepage.icon=sh-karakeep-light
      - homepage.href=https://kara.d3adc3ii.cc/
  chrome:
    image: gcr.io/zenika-hub/alpine-chrome:123
    restart: unless-stopped
    command:
      - --no-sandbox
      - --disable-gpu
      - --disable-dev-shm-usage
      - --remote-debugging-address=0.0.0.0
      - --remote-debugging-port=9222
      - --hide-scrollbars
  meilisearch:
    image: getmeili/meilisearch:v1.13.3
    restart: unless-stopped
    env_file:
      - .env
    environment:
      MEILI_NO_ANALYTICS: "true"
    volumes:
      - meilisearch:/meili_data

volumes:
  meilisearch:
  data:
"""
environment = """
KARAKEEP_VERSION=release
NEXTAUTH_SECRET=[[KARA_NEXTAUTH_SECRET]]
MEILI_MASTER_KEY=[[KARA_MEILI_MASTER_KEY]]
NEXTAUTH_URL='https://kara.d3adc3ii.cc'
OPENAI_API_KEY=[[OPENAI_API_KEY]]
OAUTH_ALLOW_DANGEROUS_EMAIL_ACCOUNT_LINKING= true
OAUTH_WELLKNOWN_URL='https://auth.d3adc3ii.cc/application/o/karakeep/'
OAUTH_CLIENT_SECRET=db64i7QZ2MHMVkjOvZfcOyWTh1y1ECG2XNamIkLXBZxi0UqLfcfvVxYcsQ7nWbZvFeRQBGnJB2vnVqGQgKkUopKGlGooRuGW2R8DnvxoORYAVWhW77x5PVhwlOB0Cr9s
OAUTH_CLIENT_ID=D8rw3kVA932oWoPiLud8z5FgomI3sXT4oOTYMBIr
OAUTH_PROVIDER_NAME= authentik
"""

##

[[stack]]
name = "kener"
tags = [
  "monitoring",
  "public-70",
  "testing",
  "3173"
]
[stack.config]
server = "Public-70"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  default:
  d3internal:
    external: true
services:
  app:
    image: rajnandan1/kener:latest   
    container_name: kener
    networks:
      - d3internal
    ports:
      - '3173:3000/tcp'
    volumes:
      - ${DOCKER_DATA}/kener/database:/app/database
      - ${DOCKER_DATA}/kener/uploads:/app/uploads              
    restart: unless-stopped
    env_file:
      - .env
"""
environment = """
TZ=Asia/Singapore
PGID=1000
PUID=1000
KENER_SECRET_KEY=qhDyycpTOv3gIhqLA0qermzMyaTyj30N

# For SQLite database...
DATABASE_URL=sqlite://./database/kener.sqlite.db
KENER_BASE_PATH=""
ORIGIN=https://kener.d3adc3ii.cc

RESEND_API_KEY=""
RESEND_SENDER_EMAIL=d3tech@pm.me
"""

##

[[stack]]
name = "lunalytics"
tags = [
  "komodo-3",
  "internal",
  "monitoring"
]
[stack.config]
server = "Public-70"
links = [
  "https://lunalytics.d3adc3ii.site"
]
project_name = "lunalytics"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
# docker-compose.yml
services:
  lunalytics:
    image: ksjaay/lunalytics:latest
    container_name: lunalytics
    restart: unless-stopped
    env_file:
      - .env
    ports:
      - '2308:2308'
    volumes:
      - ${DOCKER_DATA}/lunalytics/data:/app/data
      - ${DOCKER_DATA}/lunalytics/logs:/app/logs
    labels:
      - homepage.group=Monitor
      - homepage.name=Lunalytics
      - homepage.icon=sh-lunalytics-light
      - homepage.href=https://lunalytics.d3adc3ii.site
"""
environment = """
PGUI=1000
PUID=1000
"""

##

[[stack]]
name = "mailrise"
tags = ["komodo-3"]
[stack.config]
server = "Public-70"
file_contents = """
services:
  mailrise:
    image: yoryan/mailrise:latest
    container_name: mailrise
    ports:
      - '8025:8025'
    restart: unless-stopped
    volumes:
      -  /mnt/zApps/mailrise/mailrise.conf:/etc/mailrise.conf:ro
"""
environment = """

"""

##

[[stack]]
name = "mazanoke"
[stack.config]
server = "k-121"
file_contents = """
services:
  mazanoke:
    container_name: mazanoke
    image: ghcr.io/civilblur/mazanoke:latest
    ports:
      - "3474:80"
"""

##

[[stack]]
name = "meegoreng"
[stack.config]
server = "komodo-1"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  roti:
    image: jellydeck/moocup:latest
    container_name: meegoreng 
    ports:
      - 8811:80
    labels:
      - homepage.group= Media
      - homepage.name= meegoreng
      - homepage.icon= sh-moocup-light
      - homepage.href= https://meegoreng.d3adc3ii.site
"""

##

[[stack]]
name = "mktxp-stack"
[stack.config]
server = "Public-70"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
files_on_host = true
run_directory = "/etc/komodo/stacks/mktxp-stack"
file_paths = [
  "docker-compose-mktxp-stack.yml"
]

##

[[stack]]
name = "netalertx"
tags = [
  "internal",
  "monitoring",
  "komodo-1"
]
[stack.config]
server = "komodo-1"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  netalertx:
    container_name: netalertx
    image: "ghcr.io/jokob-sk/netalertx:latest"      
    network_mode: "host"     
    restart: unless-stopped
    volumes:
      - ${DATA_DIR}/config:/app/config
      - ${DATA_DIR}/db:/app/db      
      - ${DATA_DIR}/logs:/app/log
      - type: tmpfs
        target: /app/api
    env_file:
      - .env
    labels:
      proxy.port: 20184
    dns:           
      - 10.10.10.16
"""
environment = """
DATA_DIR=/mnt/zApps/netalertx
PUID=1000
PGID=1000
TZ=Asia/Singapore
PORT=20184
"""

##

[[stack]]
name = "newt-120"
tags = ["k-120"]
[stack.config]
server = "k-120"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  newt:
    image: fosrl/newt
    container_name: newt
    restart: unless-stopped
    environment:
      - PANGOLIN_ENDPOINT=https://pangolin.d3adc3ii.cc
      - NEWT_ID=r8kglxwqxrph1s2
      - NEWT_SECRET=0rcrulypca8o98gy7f6qs3y0vudhxl41n0ns2pyqvih9wu77
"""

##

[[stack]]
name = "newt-70"
[stack.config]
server = "Public-70"
file_contents = """
services:
  newt:
    image: fosrl/newt
    container_name: newt
    restart: unless-stopped
    environment:
      - PANGOLIN_ENDPOINT=https://pangolin.d3adc3ii.cc
      - NEWT_ID=uyuo9c5gmuzvhp7
      - NEWT_SECRET=t9gi8iprugz2wer4di52w8oahdw730z3cl4xvzcepyycd138
"""

##

[[stack]]
name = "newt-pangolin-cloud"
[stack.config]
server = "komodo-2"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  newt:
    image: fosrl/newt
    container_name: newt
    restart: unless-stopped
    env_file:
      - .env
"""
environment = """
PANGOLIN_ENDPOINT=https://pangolin.fossorial.io
NEWT_ID=1m9s5lj8p9tktns
NEWT_SECRET=ldt517vm81d34hotlycyy9bxzjfhxhwzivtdkbf2v0elrnso
"""

##

[[stack]]
name = "ns0"
[stack.config]
server = "Public-70"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  ns1:
    image: "technitium/dns-server:latest"
    container_name: ns0
    privileged: true
    restart: always
    environment:
      - DNS_SERVER_LOG_USING_LOCAL_TIME=true
    network_mode: "host"
    ports:
      - "5380:5380/tcp"
    #  - "53:53/udp" #DNS service
    #  - "53:53/tcp" #DNS service
      # - "853:853/udp" #DNS-over-QUIC service
    #  - "853:853/tcp" #DNS-over-TLS service
      # - "443:443/udp" #DNS-over-HTTPS service (HTTP/3)
      # - "443:443/tcp" #DNS-over-HTTPS service (HTTP/1.1, HTTP/2)
    #  - "80:80/tcp" #DNS-over-HTTP service (use with reverse proxy or certbot certificate renewal)
    #  - "8053:8053/tcp" #DNS-over-HTTP service (use with reverse proxy)
      # - "67:67/udp" #DHCP service
    volumes:
      - ./technitium:/etc/dns
  #keepalived:
   # image: shawly/keepalived:edge-7f210c3
    #restart: always
    #environment:
     # TZ: Asia/Singapore
      #KEEPALIVED_VIRTUAL_IP: ${VIRTUAL_IP}
#      KEEPALIVED_VIRTUAL_MASK: 24
 #     KEEPALIVED_CHECK_IP: ${CHECK_IP}
  #    KEEPALIVED_CHECK_PORT: ${CHECK_PORT}
   #   KEEPALIVED_VRID: 150
      # change to primary LAN interface used by the host
    #  KEEPALIVED_STATE: MASTER
    #network_mode: host
    #cap_add:
     # - NET_ADMIN
      #- NET_BROADCAST
"""
environment = """
# change to an unused IP in your LAN subnet -- will be the same value on both stacks
VIRTUAL_IP=10.10.10.16
# Host IP of the other machine
CHECK_IP=10.10.10.25
CHECK_PORT=53
KEEPALIVED_INTERFACE= eth0
KEEPALIVED_PRIORITY= 255
#KEEPALIVED_ROUTER_ID Keepalived virtual router ID. Defaults to 51

"""

##

[[stack]]
name = "ns1"
tags = ["internal", "komodo-1", "dns"]
[stack.config]
server = "Public-70"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  ns1:
    image: "technitium/dns-server:latest"
    container_name: ns1
    privileged: true
    restart: always
    environment:
      - DNS_SERVER_LOG_USING_LOCAL_TIME=true
    network_mode: "host"
    ports:
      - "5380:5380/tcp"
    #  - "53:53/udp" #DNS service
    #  - "53:53/tcp" #DNS service
      # - "853:853/udp" #DNS-over-QUIC service
    #  - "853:853/tcp" #DNS-over-TLS service
      # - "443:443/udp" #DNS-over-HTTPS service (HTTP/3)
      # - "443:443/tcp" #DNS-over-HTTPS service (HTTP/1.1, HTTP/2)
    #  - "80:80/tcp" #DNS-over-HTTP service (use with reverse proxy or certbot certificate renewal)
    #  - "8053:8053/tcp" #DNS-over-HTTP service (use with reverse proxy)
      # - "67:67/udp" #DHCP service
    volumes:
      - ./technitium:/etc/dns
  keepalived:
    image: shawly/keepalived:edge-7f210c3
    restart: always
    environment:
      TZ: Asia/Singapore
      KEEPALIVED_VIRTUAL_IP: ${VIRTUAL_IP}
      KEEPALIVED_VIRTUAL_MASK: 24
      KEEPALIVED_CHECK_IP: ${CHECK_IP}
      KEEPALIVED_CHECK_PORT: ${CHECK_PORT}
      KEEPALIVED_VRID: 150
      # change to primary LAN interface used by the host
      KEEPALIVED_STATE: BACKUP
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_BROADCAST
"""
environment = """
# change to an unused IP in your LAN subnet -- will be the same value on both stacks
VIRTUAL_IP=10.10.10.16
# Host IP of the other machine
CHECK_IP=10.10.10.70
CHECK_PORT=53
KEEPALIVED_INTERFACE= eth0
KEEPALIVED_PRIORITY= 100
"""

##

[[stack]]
name = "ntfy"
tags = [
  "internal",
  "monitoring",
  "komodo-2"
]
[stack.config]
server = "komodo-2"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  ntfy:
    image: binwiederhier/ntfy
    container_name: ntfy
    command:
      - serve
    environment:
      - TZ=Asia/Singapore    # optional: set desired timezone
    user: 1000:1000 # optional: replace with your own user/group or uid/gid
    env_file:
      - .env
    volumes:
      - /var/cache/ntfy:/var/cache/ntfy
      - ${NTFY_DATA_DIR}:/etc/ntfy
    ports:
      - 8193:80
    healthcheck: # optional: remember to adapt the host:port to your environment
        test: ["CMD-SHELL", "wget -q --tries=1 http://localhost:80/v1/health -O - | grep -Eo '\"healthy\"\\s*:\\s*true' || exit 1"]
        interval: 60s
        timeout: 10s
        retries: 3
        start_period: 40s
    restart: unless-stopped
"""
environment = """
NTFY_DATA_DIR=/mnt/zApps/ntfy
"""

##

[[stack]]
name = "nzbget"
tags = ["internal", "komodo-3"]
[stack.config]
server = "Public-70"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  nzbget:
    image: lscr.io/linuxserver/nzbget:latest
    container_name: nzbget
    env_file:
      - .env
    volumes:
      - ${DOCKER_DATA}/nzb/config:/config
      - ${DOCKER_DATA}/nzb/downloads:/downloads #optional
    ports:
      - 6789:6789
    restart: unless-stopped
    labels:
      proxy.idle_timeout: 10m
"""
environment = """
NZBGET_USER= [[NZBGET_USER]]
NZBGET_PASS= [[NZBGET_PASS]]
PUID= [[PUID]]
PGID= [[PGID]]
TZ= [[TZ]]
"""

##

[[stack]]
name = "pangolin"
tags = ["external"]
[stack.config]
server = "pangolin"
links = ["https://pangolin.d3adc3ii.cc"]
poll_for_updates = true
auto_update = true
auto_update_all_services = true
files_on_host = true
run_directory = "/home/d3"
file_paths = ["docker-compose.yml"]
environment = """
CLOUDFLARE_DNS_API_TOKEN= [[CF_API_TOKEN_PANGOLIN]]
"""

##

[[stack]]
name = "phpipam"
tags = ["internal", "komodo-2", "VoLM"]
[stack.config]
server = "komodo-2"
project_name = "phpipam"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  phpipam-web:
    image: phpipam/phpipam-www:latest
    container_name: phpipam
    ports:
      - "8087:80"
    restart: always
    env_file:
      - .env
    volumes:
      - ${DOCKER_DATA}/phpipam/logo:/phpipam/css/images/logo
      - ${DOCKER_DATA}/phpipam/ca:/usr/local/share/ca-certificates:ro
    depends_on:
      - phpipam-mariadb
  phpipam-cron:
    image: phpipam/phpipam-cron:latest
    env_file:
      - .env
    restart: always
    volumes:
      - ${DOCKER_DATA}/phpipam/ca:/usr/local/share/ca-certificates:ro
    depends_on:
      - phpipam-mariadb
  phpipam-mariadb:
    image: mariadb:latest
    env_file:
      - .env
    restart: always
    volumes:
      - /etc/komodo/stacks/phpipam/db:/var/lib/mysql
"""
environment = """
PUID=1000
PGID=1000
TZ=Asia/Singapore
IPAM_DATABASE_HOST=phpipam-mariadb
#IPAM_DATABASE_PASS=GnH&dXUVth5LEeItTTGQvds
IPAM_DATABASE_PASS=password
IPAM_DATABASE_WEBHOST=%

IPAM_TRUST_X_FORWARDED=true
SCAN_INTERVAL=1h
MYSQL_ROOT_PASSWORD=16E@j4pWjtL$X3Gnxfy3KQB0U$
"""

##

[[stack]]
name = "scrutiny"
tags = ["komodo-3"]
[stack.config]
server = "komodo-2"
links = [
  "https://scrutiny.d3adc3ii.cc/web/dashboard"
]
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  scrutiny-monitoring: # A common network for all monitoring services to communicate into
    external: true
  scrutiny-notification: # To Gotify or another Notification service
    external: true

services:
  influxdb:
    container_name: influxdb
    image: influxdb:2.1-alpine
    env_file:
      - .env
    user: 1000:1000
    ports:
      - 8086:8086
    volumes:
      - ${DOCKER_DATA}/scrutiny/influxdb2/db:/var/lib/influxdb2
      - ${DOCKER_DATA}/scrutiny/influxdb2/config:/etc/influxdb2
    restart: unless-stopped
    networks:
      - scrutiny-monitoring

  scrutiny:
    container_name: scrutiny
    image: ghcr.io/analogj/scrutiny:master-web
    ports:
      - 8785:8080
    volumes:
      - ${DOCKER_DATA}/scrutiny/config:/opt/scrutiny/config
    env_file:
      - .env
      # Optional but highly recommended to notify you in case of a problem
     # - SCRUTINY_NOTIFY_URLS=["http://gotify:80/message?token=a-gotify-token"]
    depends_on:
      - influxdb
    restart: unless-stopped
    networks:
      - scrutiny-notification
      - scrutiny-monitoring
"""
environment = """
PGID= 1000 
PUID= 1000
DOCKER_INFLUXDB_INIT_MODE=setup
DOCKER_INFLUXDB_INIT_USERNAME=Admin
DOCKER_INFLUXDB_INIT_PASSWORD= 'qA6otE2(raC=MDh$ZN%tNulHp6iYs|xv}l&IwVWPzZvn+Tvm399+2w#V$P'
#DOCKER_INFLUXDB_INIT_PASSWORD= [[SCRUTINY_DOCKER_INFLUXDB_INIT_PASSWORD]]
DOCKER_INFLUXDB_INIT_ORG=homelab
DOCKER_INFLUXDB_INIT_BUCKET=scrutiny
DOCKER_INFLUXDB_INIT_ADMIN_TOKEN= '6-zG/W<%eE-)rG6Gh9@WY^v#+y3KN()tf2}UNkXN=j{lDnu+P7+Kl!$G<(x['
#DOCKER_INFLUXDB_INIT_ADMIN_TOKEN= [[SCRUTINY_DOCKER_INFLUXDB_INIT_ADMIN_TOKEN]]
SCRUTINY_WEB_INFLUXDB_HOST=influxdb
SCRUTINY_WEB_INFLUXDB_PORT=8086
SCRUTINY_WEB_INFLUXDB_TOKEN= '6-zG/W<%eE-)rG6Gh9@WY^v#+y3KN()tf2}UNkXN=j{lDnu+P7+Kl!$G<(x['
#SCRUTINY_WEB_INFLUXDB_TOKEN= [[SCRUTINY_WEB_INFLUXDB_TOKEN]]
SCRUTINY_WEB_INFLUXDB_ORG=homelab
SCRUTINY_WEB_INFLUXDB_BUCKET=scrutiny
"""

##

[[stack]]
name = "silverbullet"
tags = ["external", "komodo-2"]
[stack.config]
server = "komodo-2"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  silverbullet:
    image: ghcr.io/silverbulletmd/silverbullet:v2
    container_name: silverbullet
    restart: unless-stopped
    env_file:
      - .env
    volumes:
      - ${DOCKER_DATA}/silverbullet/space:/space
    ports:
      - 3718:3000
    labels:
      proxy.exclude: true
"""
environment = """
SB_USER=[[SILVERBULLET_USER]]
PGID=1000
PUID=1000
"""

##

[[stack]]
name = "spheressl"
[stack.config]
server = "Public-70"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  default:
  d3internal:
    external: true
services:
  spheressl:
    image: kl3mta3/spheressl:latest
    container_name: spheressl
    networks:
      - default
      - d3internal
    ports:
      - "7171:7171"
    volumes:
      - ${DOCKER_DATA}/spheressl/data:/app/data
      - ${DOCKER_DATA}/spheressl/certs:/app/certs
      - ${DOCKER_DATA}/spheressl/logs:/app/logs
    env_file:
      - .env
    environment:
      - ASPNETCORE_ENVIRONMENT=Production
      - PGID=1000
      - PUID=1000
      - ServerIP=0.0.0.0
      - ServerPort=7171
    restart: unless-stopped
    labels:
      proxy.network: default
"""
environment = """
TZ=Asia/Singapore
PGID=1000
PUID=1000
SECRET=
"""

##

[[stack]]
name = "stashapp"
tags = ["komodo-2"]
[stack.config]
server = "komodo-2"
links = ["https://stash.d3adc3ii.site"]
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
# APPNICENAME=Stash
# APPDESCRIPTION=An organizer for your porn, written in Go
services:
  stash:
    image: stashapp/stash:latest
    container_name: stash
    restart: unless-stopped
    ## the container's port must be the same with the STASH_PORT in the environment section
    ports:
      - "9999:9999"
    ## If you intend to use stash's DLNA functionality uncomment the below network mode and comment out the above ports section
    # network_mode: host
    logging:
      driver: "json-file"
      options:
        max-file: "10"
        max-size: "2m"
    env_file:
      - .env
    environment:
      - PUID=1000
      - PGID=1000
      - STASH_STASH=/data/
      - STASH_GENERATED=/generated/
      - STASH_METADATA=/metadata/
      - STASH_CACHE=/cache/
      ## Adjust below to change default port (9999)
      - STASH_PORT=9999
    volumes:
      - /etc/localtime:/etc/localtime:ro
      ## Adjust below paths (the left part) to your liking.
      ## E.g. you can change ./config:/root/.stash to ./stash:/root/.stash
      ## The left part is the path on your host, the right part is the path in the stash container.

      ## Keep configs, scrapers, and plugins here.
      - ${DOCKER_DATA}/stashapp/config:/root/.stash
      ## Point this at your collection./stashapp/config
      ## The left side is where your collection is on your host, the right side is where it will be in stash.
      - ${VAULT}/vault:/data
      ## This is where your stash's metadata lives
      - ${DOCKER_DATA}/stashapp/metadata:/metadata
      ## Any other cache content.
      - ${DOCKER_DATA}stashapp/cache:/cache
      ## Where to store binary blob data (scene covers, images)
      - ${DOCKER_DATA}/stashapp/blobs:/blobs
      ## Where to store generated content (screenshots,previews,transcodes,sprites)
      -  ${DOCKER_DATA}stashapp/generated:/generated
    labels:
      - homepage.group=Media
      - homepage.name=Stash
      - homepage.icon=sh-stashapp-light
      - homepage.href=https://stash.d3adc3ii.site
      #- homepage.widget.type=
      #- homepage.widget.widget.url=
      #- homepage.widget.stashapikey=
"""
environment = """
VAULT=/mnt/zFiles
"""

##

[[stack]]
name = "template"
template = true
[stack.config]
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  default:
  d3internal:
    external: true
services:
  app:
    image:
    container_name: 
    networks:
      - d3internal
    ports:
      - :
    volumes:
      - ${DOCKER_DATA}/           
    restart: unless-stopped
    env_file:
      - .env
    
    labels:
#### Database if any ####################################
  postgres:
    image: postgres:alpine
    container_name: postgres
    env_file:
      - .env
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: some_super_random_secure_password    
      POSTGRES_DB: postgres
    restart: unless-stopped
"""
environment = """
TZ=Asia/Singapore
PGID=1000
PUID=1000
SECRET=
"""

##

[[stack]]
name = "termix"
[stack.config]
server = "Public-70"
links = ["https://termix.d3adc3ii.cc"]
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  d3internal:
    external: true
services:
  termix:
    image: ghcr.io/lukegus/termix:latest
    container_name: termix
    restart: unless-stopped
    networks:
      - d3internal
    ports:
      - "8282:8080"
    volumes:
      - ${DOCKER_DATA}/termix/data:/app/data
    env_file:
      - .env
    labels:
      - homepage.group=Access
      - homepage.name=Termix
      - homepage.icon=sh-termix-light
      - homepage.href=https://termix.d3adc3ii.cc
"""
environment = """
SALT: "tJ,V_^12phQ=66k'<v*7G?-Qw:/o)dom"
PORT: "8080"
"""

##

[[stack]]
name = "uptime-kuma"
tags = [
  "external",
  "monitoring",
  "komodo-3"
]
[stack.config]
server = "Public-70"
links = ["https://uptime.d3adc3ii.cc/"]
project_name = "uptime-kuma"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  d3internal:
    external: true
services:
  uptime-kuma:
    image: louislam/uptime-kuma:1
    container_name: uptime-kuma
    networks: 
      - d3internal
    env_file:
      - .env
    volumes:
      - ${DOCKER_DATA}/uptime-kuma/data:/app/data
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - 3111:3001
    restart: always
    labels:
      - homepage.group=Monitor
      - homepage.name=Uptime Kuma
      - homepage.icon=sh-uptime-kuma-light
      - homepage.href=https://uptime.d3adc3ii.cc/
      - homepage.widget.type=uptimekuma
      - homepage.widget.url=https://uptime.d3adc3ii.cc
      - homepage.widget.slug=ping
"""
environment = """
PUID=1000
PGID=1000
"""

##

[[stack]]
name = "wallos"
tags = [
  "external",
  "production",
  "komodo-3"
]
[stack.config]
server = "komodo-1"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  d3internal:
    external: true
services:
  wallos:
    container_name: wallos
    image: bellamy/wallos:latest
    networks:
      - d3internal
    ports:
      - "8282:80/tcp"
    env_file:
      - .env
    volumes:
      - ${DATA_DIR}/db:/var/www/html/db'
      - ${DATA_DIR}/logos:/var/www/html/images/uploads/logos'
    restart: unless-stopped
    labels:
      - homepage.group=Applications
      - homepage.name=Wallos
      - homepage.icon=sh-wallos-light
      - homepage.href=https://wallos.d3adc3ii.cc
"""
environment = """
TZ= 'Asia/Singapore'
DATA_DIR=/mnt/zApps/wallos
PUID=1000
PGID=1000
"""

##

[[stack]]
name = "zitadel"
[stack.config]
server = "Public-70"
destroy_before_deploy = true
additional_env_files = ["db.env"]
file_contents = """
services:
  zitadel:
    user: "${UID:-1000}"
    restart: 'always'
    networks:
      - 'zitadel'
    command: 'start-from-init --masterkey "MasterkeyNeedsToHave32Characters" --tlsMode disabled'
    depends_on:
      db:
        condition: 'service_healthy'
    image: ghcr.io/zitadel/zitadel:latest
    env_file:
      - .env 
    healthcheck:
      test:
      - CMD
      - /app/zitadel
      - ready
      interval: 10s
      timeout: 60s
      retries: 5
      start_period: 10s
    volumes:
      - .:/current-dir:delegated
    ports:
      - 8080:8080
      - 3000:3000


  login:
    restart: unless-stopped
    image: ghcr.io/zitadel/zitadel-login:latest
    # If you can't use the network_mode service:zitadel, you can pass the environment variable CUSTOM_REQUEST_HEADERS=Host:localhost instead.
    environment:
      - ZITADEL_API_URL=http://localhost:8080
      - NEXT_PUBLIC_BASE_PATH=/ui/v2/login
      - ZITADEL_SERVICE_USER_TOKEN_FILE=/current-dir/login-client.pat
    user: "${UID:-1000}"
    network_mode: service:zitadel
    volumes:
      - .:/current-dir:ro
    depends_on:
      zitadel:
        condition: service_healthy
        restart: false
  db:
    restart: 'always'
    image: postgres:16-alpine
    env_file:
      - ./db.env
    networks:
      - 'zitadel'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready", "-d", "zitadel", "-U", "postgres", "-c", "log_statement=all"]
      interval: '10s'
      timeout: '30s'
      retries: 5
      start_period: '20s'
    volumes:
      - db-data:/var/lib/postgresql/data

networks:
  zitadel:
volumes:
  db-data:
"""
environment = """
      ZITADEL_EXTERNALSECURE: false
      ZITADEL_TLS_ENABLED: false
      ZITADEL_DATABASE_POSTGRES_HOST: db
      ZITADEL_DATABASE_POSTGRES_PORT: 5432
      ZITADEL_DATABASE_POSTGRES_DATABASE: zitadel
      ZITADEL_DATABASE_POSTGRES_USER_USERNAME: zitadeluser
      ZITADEL_DATABASE_POSTGRES_USER_PASSWORD: zitadel123
      ZITADEL_DATABASE_POSTGRES_USER_SSL_MODE: disable
      ZITADEL_DATABASE_POSTGRES_ADMIN_USERNAME: postgres
      ZITADEL_DATABASE_POSTGRES_ADMIN_PASSWORD: postgres
      ZITADEL_DATABASE_POSTGRES_ADMIN_SSL_MODE: disable
      # By configuring a login client, the setup job creates a user of type machine with the role IAM_LOGIN_CLIENT.
      # It writes a PAT to the path specified in ZITADEL_FIRSTINSTANCE_LOGINCLIENTPATPATH.
      # The PAT is passed to the login container via the environment variable ZITADEL_SERVICE_USER_TOKEN_FILE.
      ZITADEL_FIRSTINSTANCE_LOGINCLIENTPATPATH: /current-dir/login-client.pat
      ZITADEL_FIRSTINSTANCE_ORG_HUMAN_PASSWORDCHANGEREQUIRED: false
      ZITADEL_FIRSTINSTANCE_ORG_LOGINCLIENT_MACHINE_USERNAME: login-client
      ZITADEL_FIRSTINSTANCE_ORG_LOGINCLIENT_MACHINE_NAME: Automatically Initialized IAM_LOGIN_CLIENT
      ZITADEL_FIRSTINSTANCE_ORG_LOGINCLIENT_PAT_EXPIRATIONDATE: '2029-01-01T00:00:00Z'
      ZITADEL_DEFAULTINSTANCE_FEATURES_LOGINV2_REQUIRED: true
      ZITADEL_DEFAULTINSTANCE_FEATURES_LOGINV2_BASEURI: http://localhost:3000/ui/v2/login
      ZITADEL_OIDC_DEFAULTLOGINURLV2: http://localhost:3000/ui/v2/login/login?authRequest=
      ZITADEL_OIDC_DEFAULTLOGOUTURLV2: http://localhost:3000/ui/v2/login/logout?post_logout_redirect=
      ZITADEL_SAML_DEFAULTLOGINURLV2: http://localhost:3000/ui/v2/login/login?samlRequest=
      # By configuring a machine, the setup job creates a user of type machine with the role IAM_OWNER.
      # It writes a personal access token (PAT) to the path specified in ZITADEL_FIRSTINSTANCE_PATPATH.
      # The PAT can be used to provision resources with [Terraform](/docs/guides/manage/terraform-provider), for example.
      ZITADEL_FIRSTINSTANCE_PATPATH: /current-dir/admin.pat
      ZITADEL_FIRSTINSTANCE_ORG_MACHINE_MACHINE_USERNAME: admin
      ZITADEL_FIRSTINSTANCE_ORG_MACHINE_MACHINE_NAME: Automatically Initialized IAM_OWNER
      ZITADEL_FIRSTINSTANCE_ORG_MACHINE_MACHINEKEY_TYPE: 1
"""

##

[[deployment]]
name = "apprise"
[deployment.config]
image.type = "Image"
image.params.image = "caronc/apprise:latest"
poll_for_updates = true
auto_update = true
network = "bridge"
ports = """
8000:8000
"""
volumes = """
/mnt/zApps/apprise_api/config:/opt/apprise/config
/mnt/zApps/apprise_api/plugin:/opt/apprise/plugin
/mnt/zApps/apprise_api/attach:/opt/apprise/attach
"""
environment = """
PUID=1000
PGID=1000
APPRISE_STATEFUL_MODE=simple
APPRISE_WORKER_COUNT=1
"""

##

[[deployment]]
name = "nessus"
tags = ["internal", "testing"]
[deployment.config]
server = "komodo-2"
image.type = "Image"
image.params.image = "tenable/nessus:latest-ubuntu"

##

[[deployment]]
name = "statementsensei"
tags = ["internal", "testing"]
[deployment.config]
server = "komodo-2"
image.type = "Image"
image.params.image = "benjaminawd/statementsensei:latest"
poll_for_updates = true
auto_update = true
network = "bridge"
ports = """
8501:8501
"""
environment = """
  PDF_PASSWORD= [[PDF_PASSWORD]]
"""
labels = """
caddy: statement.d3adc3ii.site
caddy.reverse_proxy: "{{upstreams 8501}}"
"""

##

[[deployment]]
name = "twingate-70"
tags = ["external", "production"]
[deployment.config]
image.type = "Image"
image.params.image = "twingate/connector:latest"
poll_for_updates = true
auto_update = true
restart = "unless-stopped"
environment = """
  TWINGATE_NETWORK=d3net
  TWINGATE_ACCESS_TOKEN=eyJhbGciOiJFUzI1NiIsImtpZCI6Ik5RejJ2eDFHR2hQeXR6OHlxS1ZWbDUwMUNTSzlOQUpMcUV2ZWpyQ0d5N0UiLCJ0eXAiOiJEQVQifQ.eyJudCI6IkFOIiwiYWlkIjoiNTc1NDc4IiwiZGlkIjoiMjQyNjc0NSIsInJudyI6MTc1NDc5Njk0NCwianRpIjoiNzM3NWU3NTUtODAwMS00NzZjLWE0ZTItNDg3YmMzMDZkOTFiIiwiaXNzIjoidHdpbmdhdGUiLCJhdWQiOiJkM25ldCIsImV4cCI6MTc1NDgwMDIwNiwiaWF0IjoxNzU0Nzk2NjA2LCJ2ZXIiOiI0IiwidGlkIjoiMTAzNTY0Iiwicm5ldGlkIjoiMTM1OTMxIn0.-F9bRC-CrpQIzYcn58m2owGzTI70XX4s_6yUMu8Lm7bgAcYJC7WZgcCgoxHxEttiQPQPZ6ixaHBdwIw2rdwRrg
  TWINGATE_REFRESH_TOKEN=kuIJdK232YHEXtPvru4WzvMgK6cCi4ZcUe5ohTTq6dRQkQnAbizlhxIpQvX1bKOTRvPARXMcKhnqLFfLxUIKaoXNNHfRe9SdJioFgPEZQU3geYafa4CO9Isn5HiATd1QwFJn1w
  TWINGATE_LOG_ANALYTICS=v2
"""

##

[[repo]]
name = "Public-70"
[repo.config]
server = "Public-70"
git_account = "d3hl"
repo = "d3hl/dIIIhl"

##

[[repo]]
name = "diiihl"
[repo.config]
server = "komodo-1"
builder = "local"
git_account = "d3hl"
repo = "d3hl/dIIIhl"

##

[[repo]]
name = "diiihl-3"
[repo.config]
builder = "local"
git_account = "d3hl"
repo = "d3hl/dIIIhl"

##

[[procedure]]
name = "Backup Core Database"
description = "Triggers the Core database backup at the scheduled time."
tags = ["system"]
config.schedule = "Every day at 01:00"

[[procedure.config.stage]]
name = "Stage 1"
enabled = true
executions = [
  { execution.type = "BackupCoreDatabase", execution.params = {}, enabled = true }
]

##

[[procedure]]
name = "Sync from Git, pull repo and deploy Homepage"
tags = ["homepage"]

[[procedure.config.stage]]
name = "Stage 4"
enabled = true
executions = [
  { execution.type = "RunSync", execution.params.sync = "sync", enabled = true }
]

[[procedure.config.stage]]
name = "pull repo and destroy stack"
enabled = true
executions = [
  { execution.type = "PullRepo", execution.params.repo = "diiihl", enabled = true },
  { execution.type = "DestroyStack", execution.params.stack = "homepage", execution.params.services = [], execution.params.remove_orphans = false, enabled = false }
]

[[procedure.config.stage]]
name = "Stage 3"
enabled = true
executions = [
  { execution.type = "DeployStackIfChanged", execution.params.stack = "homepage", enabled = true }
]

##

[[procedure]]
name = "commit sync ( UI > Repo)"

[[procedure.config.stage]]
name = "Stage 2"
enabled = true
executions = [
  { execution.type = "CommitSync", execution.params.sync = "sync", enabled = true }
]

##

[[alerter]]
name = "discord-webhook"
[alerter.config]
enabled = true
endpoint.type = "Custom"
endpoint.params.url = "http://10.10.10.26:7000"
alert_types = [
  "ServerUnreachable",
  "ServerMem",
  "ServerCpu",
  "ServerDisk",
  "StackAutoUpdated",
  "StackStateChange",
  "ContainerStateChange",
  "StackImageUpdateAvailable",
  "DeploymentAutoUpdated",
  "DeploymentImageUpdateAvailable",
  "ResourceSyncPendingUpdates",
  "AwsBuilderTerminationFailed",
  "BuildFailed",
  "RepoBuildFailed"
]

##

[[builder]]
name = "local"
[builder.config]
type = "Server"
params = {}

##

[[resource_sync]]
name = "sync"
[resource_sync.config]
repo = "d3hl/dIIIhl"
git_account = "d3hl"
webhook_secret = "ZDuIhK3CjhALwwkaZR8jiclMG8Z3xGbS"
resource_path = ["komodo/resources/main.toml"]
managed = true
include_user_groups = true

##

[[resource_sync]]
name = "var"
[resource_sync.config]
repo = "d3hl/dIIIhl"
git_account = "d3hl"
resource_path = ["komodo/resources/var.toml"]
managed = true
include_resources = false
include_variables = true