[[server]]
name = "komodo-2"
[server.config]
address = "https://192.168.2.32:8120"
enabled = true

##

[[server]]
name = "komodo-3"
[server.config]
address = "https://192.168.2.33:8120"
enabled = true

##

[[server]]
name = "server-bkxal"
[server.config]
address = "https://periphery:8120"
enabled = true

##

[[stack]]
name = "actual-budget"
tags = ["internal", "production"]
[stack.config]
server = "komodo-2"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  caddy:
    external: true

services:
  actual_server:
    image: docker.io/actualbudget/actual-server:latest
    ports:
      - '5006:5006'
    networks:
      - caddy
    volumes:
      - ${ACTUAL_DATA_DIR}:/data
    healthcheck:
      # Enable health check for the instance
      test: ['CMD-SHELL', 'node src/scripts/health-check.js']
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: unless-stopped
    labels:
      caddy: actual.d3adc3ii.site
      caddy.reverse_proxy: "{{upstreams 5006}}"
"""
environment = """
ACTUAL_DATA_DIR=/mnt/zApps/actualbudget
"""

##

[[stack]]
name = "authentik"
tags = ["external", "production"]
[stack.config]
server = "server-bkxal"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  postgresql:
    image: docker.io/library/postgres:16-alpine
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}"]
      start_period: 20s
      interval: 30s
      retries: 5
      timeout: 5s
    volumes:
      - database:/var/lib/postgresql/data
    environment:
      POSTGRES_PASSWORD: ${PG_PASS}
      POSTGRES_USER: ${PG_USER:-authentik}
      POSTGRES_DB: ${PG_DB:-authentik}
    env_file:
      - .env
  redis:
    image: docker.io/library/redis:alpine
    command: --save 60 1 --loglevel warning
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      start_period: 20s
      interval: 30s
      retries: 5
      timeout: 3s
    volumes:
      - redis:/data
  server:
    image: ${AUTHENTIK_IMAGE:-ghcr.io/goauthentik/server}:${AUTHENTIK_TAG:-2025.4.1}
    restart: unless-stopped
    command: server
    environment:
      AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
      AUTHENTIK_REDIS__HOST: redis
      AUTHENTIK_POSTGRESQL__HOST: postgresql
      AUTHENTIK_POSTGRESQL__USER: ${PG_USER:-authentik}
      AUTHENTIK_POSTGRESQL__NAME: ${PG_DB:-authentik}
      AUTHENTIK_POSTGRESQL__PASSWORD: ${PG_PASS}
    volumes:
      - ${AUTHENTIK_DATA_DIR}/media:/media
      - ${AUTHENTIK_DATA_DIR}/custom-templates:/templates
    env_file:
      - .env
    ports:
      - "${COMPOSE_PORT_HTTP:-9000}:9000"
      - "${COMPOSE_PORT_HTTPS:-9443}:9443"
    depends_on:
      postgresql:
        condition: service_healthy
      redis:
        condition: service_healthy
  worker:
    image: ${AUTHENTIK_IMAGE:-ghcr.io/goauthentik/server}:${AUTHENTIK_TAG:-2025.4.0}
    restart: unless-stopped
    command: worker
    environment:
      AUTHENTIK_SECRET_KEY: ${AUTHENTIK_SECRET_KEY}
      AUTHENTIK_REDIS__HOST: redis
      AUTHENTIK_POSTGRESQL__HOST: postgresql
      AUTHENTIK_POSTGRESQL__USER: ${PG_USER:-authentik}
      AUTHENTIK_POSTGRESQL__NAME: ${PG_DB:-authentik}
      AUTHENTIK_POSTGRESQL__PASSWORD: ${PG_PASS}
    # `user: root` and the docker socket volume are optional.
    # See more for the docker socket integration here:
    # https://goauthentik.io/docs/outposts/integrations/docker
    # Removing `user: root` also prevents the worker from fixing the permissions
    # on the mounted folders, so when removing this make sure the folders have the correct UID/GID
    # (1000:1000 by default)
    user: root
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${AUTHENTIK_DATA_DIR}/media:/media
      - ./certs:/certs
      - ${AUTHENTIK_DATA_DIR}/custom-templates:/templates
    env_file:
      - .env
    depends_on:
      postgresql:
        condition: service_healthy
      redis:
        condition: service_healthy

volumes:
  database:
    driver: local
  redis:
    driver: local
"""
environment = """
AUTHENTIK_SECRET_KEY= [[AUTHENTIK_SECRET_KEY]]
PG_PASS= [[AUTHENTIK_PG_PASS]]
AUTHENTIK_DATA_DIR=/mnt/zApps/authentik
# SMTP Host Emails are sent to
AUTHENTIK_EMAIL__HOST=localhost
AUTHENTIK_EMAIL__PORT=25
# Use StartTLS
AUTHENTIK_EMAIL__USE_TLS=false
# Use SSL
AUTHENTIK_EMAIL__USE_SSL=false
AUTHENTIK_EMAIL__TIMEOUT=10
# Email address authentik will send from, should have a correct @domain
AUTHENTIK_EMAIL__FROM=authentik@localhost
"""

##

[[stack]]
name = "battle-of-the-bandwidth"
tags = [
  "internal",
  "testing",
  "monitoring"
]
[stack.config]
server = "komodo-2"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
git_account = "d3adc3iii"
repo = "d3adc3iii/dIIIhl"
run_directory = "/etc/komodo/stacks/battle-of-the-bandwidth/server12/battle-of-the-bandwidth"

##

[[stack]]
name = "caddy"
tags = ["internal", "production", "core"]
[stack.config]
server = "server-bkxal"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  caddy: 
    name: caddy
    ipam:
      driver: default  
    
# Main Caddy
services:
  caddy:
    image: homeall/caddy-reverse-proxy-cloudflare:latest
    networks:
      - caddy
    ports:
      - 80:80
      - 443:443
      - "443:443/udp"
    env_file:
      - .env
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ${CADDY_DATA_DIR}/data:/data
      - ${CADDY_DATA_DIR}/config/:/config/caddy  #ðŸ‘ˆ where to save configs 
    restart: unless-stopped
    extra_hosts:
      - host.docker.internal:host-gateway

# Caddy-config container
  caddy-config:
    container_name: caddy-config
    image: traefik/whoami:latest
    networks:
      - caddy
    restart: always
    env_file:
      - .env
    labels:
    #############################################
    # Settings and snippets to get things working
    # You shouldn't need to modify this normally
    # Custom settings and definitions are below
    #############################################

      #### Global Settings ####
      caddy_0.email: ${CADDY_EMAIL}
      caddy_0.auto_https: prefer_wildcard

      #### Snippets ####
      # Get wildcard certificate
      caddy_1: (wildcard)
      #caddy_1.acme_dns: "cloudflare ${CF_API_TOKEN}" 
      caddy_1.tls.dns: "cloudflare ${CF_API_TOKEN}"
      caddy_1.tls.resolvers: 1.1.1.1 1.0.0.1
      caddy_1.handle.abort: ""

      # Skip TLS verify for backend with self-signed HTTPS
      caddy_3: (https)
      caddy_3.transport: http
      caddy_3.transport.tls: ""
      caddy_3.transport.tls_insecure_skip_verify: ""

    ###########################################
    # Custom settings. Modify things below ðŸ‘‡:
    # Make sure they have unique label numbers
    ###########################################

      # Custom global settings, add/edit as needed
      # caddy_0.log: default
      # caddy_0.log.format: console

      # Uncomment this during testing to avoid hitting rate limit.
      # It will try to obtain SSL from Let's Encrypt's staging endpoint.
      # acme_ca: "https://acme-staging-v02.api.letsencrypt.org/directory" # ðŸ‘ˆ Staging

      ## Setup wildcard sites
      caddy_10: "*.d3adc3ii.site"   #ðŸ‘ˆ Change to your domain
      caddy_10.import: wildcard

      # Caddy Admin Endpoint and Metrics
      caddy_20: :2020
      caddy_20.handle: /reverse_proxy/upstreams
      caddy_20.handle.reverse_proxy: localhost:2019
      caddy_20.handle.reverse_proxy.header_up: Host localhost:2019
    

      # Add our first site, which this container itself
      caddy_99: whoami.d3adc3ii.site                       #ðŸ‘ˆ Subdomain using wildcard cert
      caddy_99.reverse_proxy: "{{upstreams 80}}"         #ðŸ‘ˆ Container port

      # For non-docker sites see https://gist.github.com/omltcat/241ef622070ca0580f2876a7cfa7de67
      # e.g.: Pi-Hole on another machine in the same LAN
      caddy_100: netalertx.d3adc3ii.site                      
      caddy_100.reverse_proxy: 192.168.2.31:20184

      caddy_101: pve.d3adc3ii.site                      
      caddy_101.reverse_proxy: 192.168.2.11:8006
      caddy_101.reverse_proxy.transport: http
      caddy_101.reverse_proxy.transport.tls_insecure_skip_verify: ""


      caddy_102: opn.d3adc3ii.site                      
      caddy_102.reverse_proxy: 192.168.2.1:2184
      caddy_102.reverse_proxy.transport: http
      caddy_102.reverse_proxy.transport.tls: ""
      caddy_102.reverse_proxy.transport.versions: 1.1 1.2
      caddy_102.reverse_proxy.transport.tls_insecure_skip_verify: ""

      caddy_103: truenas.d3adc3ii.site                      
      caddy_103.reverse_proxy: 192.168.2.7
      caddy_103.reverse_proxy.transport: http
      caddy_103.reverse_proxy.transport.tls_insecure_skip_verify: ""

      caddy_104: pbs.d3adc3ii.site   
      caddy_104.reverse_proxy: 192.168.2.35:8007
      caddy_104.reverse_proxy.transport: http
      caddy_104.reverse_proxy.transport.tls_insecure_skip_verify: ""

      caddy_105: ns0.d3adc3ii.site                      
      caddy_105.reverse_proxy: 192.168.2.5:5380
      caddy_106: stash.d3adc3ii.site                      
      caddy_106.reverse_proxy: 192.168.2.26:9999
      caddy_107: ha.d3adc3ii.site                      
      caddy_107.reverse_proxy: 192.168.99.5:8123    
      caddy_108: jelly.d3adc3ii.site                      
      caddy_108.reverse_proxy: 192.168.2.33:8096
      caddy_109: semaphore.d3adc3ii.site                      
      caddy_109.reverse_proxy: 192.168.2.28:3000            
      caddy_110: aria.d3adc3ii.site                      
      caddy_110.reverse_proxy: 192.168.2.16:6880  
      caddy_111: pulse.d3adc3ii.site                      
      caddy_111.reverse_proxy: 192.168.2.36:7655         
      # e.g. OpenMediaVault on the host machine, with self-signed https at port 4430
      #caddy_101: omv.example.com                         #ðŸ‘ˆ Subdomain using wildcard cert
      #caddy_101.reverse_proxy: host.docker.internal:4430 #ðŸ‘ˆ Port on host machine
      #caddy_101.reverse_proxy.import: https              #ðŸ‘ˆ Allow self-signed cert between OMV and Caddy
      #caddy_101.import: auth                             #ðŸ‘ˆ Enable protection by Authelia
"""
environment = """
PUID=1000
PGID=1000
CADDY_DATA_DIR=/mnt/zApps/caddy
CADDY_INGRESS_NETWORKS=caddy
CF_API_TOKEN= [[CF_API_TOKEN]]
CADDY_EMAIL= d3tech@pm.me
"""

##

[[stack]]
name = "dumbterm"
tags = ["external", "production"]
[stack.config]
server = "komodo-2"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  dumbterm:
    image: dumbwareio/dumbterm:latest
    container_name: dumbterm
    restart: unless-stopped
    ports:
      - ${DUMBTERM_PORT}:3000
    volumes:
      - ${DUMBTERM_CONFIG}:/root/.config
      - ${DUMBTERM_DATA_DIR}:/root/data
    environment:
      # Container timezone
      TZ: ${DUMBTERM_TZ}
      # The title shown in the web interface
      SITE_TITLE: ${DUMBTERM_SITE_TITLE:-DumbTerm}
      # Recommended PIN protection (leave empty to disable)
      DUMBTERM_PIN: ${DUMBTERM_PIN}
      # The base URL for the application
      BASE_URL: ${DUMBTERM_BASE_URL}
      ENABLE_STARSHIP: ${ENABLE_STARSHIP:-true}
      LOCKOUT_TIME: ${DUMBTERM_LOCKOUT_TIME:-15} # Minutes
      # Session duration in hours before requiring re-authentication
      MAX_SESSION_AGE: ${DUMBTERM_MAX_SESSION_AGE:-24} # Hours
      # (OPTIONAL) - List of allowed origins for CORS
      # ALLOWED_ORIGINS: ${DUMBTERM_ALLOWED_ORIGINS:-http://localhost:3000}
"""
environment = """
DUMBTERM_CONFIG="/mnt/zApps/dumbterm/config"
DUMBTERM_DATA_DIR="/mnt/zApps/dumbterm/data"
DUMBTERM_TZ="Asia/Singapore"
DUMBTERM_PIN=1111
DUMBTERM_PORT=3002
DUMBTERM_BASE_URL=http://dumbterm.d3adc3ii.cc:3002
"""

##

[[stack]]
name = "file-share"
[stack.config]
server = "komodo-2"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  caddy:
    external: true
services:
  fireshare:
    container_name: fireshare
    image: shaneisrael/fireshare:latest
    networks:
      - caddy
    ports:
      - "8585:80"
    volumes:
      - ${FSHARE_DATA_DIR}/data:/data
      - ${FSHARE_DATA_DIR}/processed:/processed
      - ${FSHARE_DATA_DIR}/videos:/videos
    env_file:
      - .env
    environment:
      - ADMIN_USERNAME=${ADMIN_USERNAME}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}
      - SECRET_KEY=${SECRET_KEY}
      - MINUTES_BETWEEN_VIDEO_SCANS=5
      # The location in the video thumbnails are generated. A value between 0-100 where 50 would be the frame in the middle of the video file and 0 would be the first frame of the video.
      - THUMBNAIL_VIDEO_LOCATION=0
      # The domain your instance is hosted at. (do not add http or https) e.x: v.fireshare.net, this is required for opengraph to work correctly for shared links. DO NOT SURROUND IN QUOTES.
      - DOMAIN=fs.d3adc3ii.cc
      - PUID=1000
      - PGID=1000
    labels:
      caddy: fs.d3adc3ii.site
      caddy.reverse_proxy: "{{upstreams 8585}}"
"""
environment = """
FSHARE_DATA_DIR=/mnt/zApps/fileshare
ADMIN_USERNAME=d3
ADMIN_PASSWORD=[[FSHARE_ADMIN_PASSWORD]]
SECRET_KEY=[[FSHARE_SECRET_KEY]]
"""

##

[[stack]]
name = "healthcheck"
[stack.config]
server = "server-bkxal"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  caddy:
    external: true

services:
  healthchecks:
    image: lscr.io/linuxserver/healthchecks:latest
    container_name: healthchecks
    networks:
      - caddy
    env_file:
      - .env
    #environment:
      #- CSRF_TRUSTED_ORIGINS= #optional
      #- DEBUG=True #optional
      #- DEFAULT_FROM_EMAIL= #optional
      #- EMAIL_HOST= #optional
      #- EMAIL_PORT= #optional
      #- EMAIL_HOST_USER= #optional
      #- EMAIL_HOST_PASSWORD= #optional
      #- EMAIL_USE_TLS= #optional
      #- INTEGRATIONS_ALLOW_PRIVATE_IPS= #optional
      #- PING_EMAIL_DOMAIN= #optional
      #- RP_ID= #optional
      #- SITE_LOGO_URL= #optional
    volumes:
      - ${DATA_DIR}/config:/config
    ports:
      - 8000:8000
      - 2525:2525 #optional
    restart: unless-stopped
    labels:
      caddy: health.d3adc3ii.site
      caddy.reverse_proxy: "{{upstreams 8000}}"
"""
environment = """
PUID=1000
PGID=1000
DATA_DIR=/mnt/zApps/healthcheck
SITE_ROOT= "https://health.d3adc3ii.site"
ALLOWED_HOSTS=health.d3adc3ii.site
SITE_NAME= Healthcheck
SUPERUSER_EMAIL= d3tech@pm.me
SUPERUSER_PASSWORD= [[HEALTHCHECK_SUPERADMIN_PASSWORD]]
SECRET_KEY= [[HEALTHCHECK_SECRET]]
APPRISE_ENABLED=True
TZ=Asia/Singapore
"""

##

[[stack]]
name = "homepage"
tags = ["external", "production"]
[stack.config]
server = "server-bkxal"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  caddy:
    external: true

services:
  homepage:
    image: ghcr.io/gethomepage/homepage:latest
    container_name: homepage
    networks:
      - caddy
    env_file:
      - .env
    ports:
      - 3000:3000
    volumes:
      - ${DATA_DIR}/config:/app/config 
      - ${DATA_DIR}/icons:/app/public/icons
      - ${DATA_DIR}/images:/app/public/images 
      - /var/run/docker.sock:/var/run/docker.sock:ro
    restart: unless-stopped
    labels:
      caddy: homepage.d3adc3ii.site                    
      caddy.reverse_proxy: "{{upstreams 3000}}"
"""
environment = """
DATA_DIR= /etc/komodo/repos/diiihl/komodo/resources/homepage
HOMEPAGE_ALLOWED_HOSTS= "homepage.d3adc3ii.cc,homepage.d3adc3ii.site,192.168.2.31:3000"
PGID= 1000 
PUID= 1000 
# Site Config
HOMEPAGE_VAR_TITLE= "d3 Homepage"
HOMEPAGE_VAR_FAVICON= "/icons/d3logo.png"
HOMEPAGE_VAR_IMG_URL= "/images/culon.jpg"

### INFRA ###
# PULSE
HOMEPAGE_VAR_PULSE_URL= "http://192.168.2.36:7655"
HOMEPAGE_VAR_PULSE_ALLOWFULLSCREEN= true
HOMEPAGE_VAR_PULSE_LOADING_STRATEGY=eager
HOMEPAGE_VAR_PULSE_ALLOW_SCROLLING=false
HOMEPAGE_VAR_PULSE_CLASSES= h-300 sm:h-60 md:h-60 lg:h-60 xl:h-60 2xl:h-72 

# Truenas
HOMEPAGE_VAR_TRUENAS_ICON= "/icons/truenas.png"
HOMEPAGE_VAR_TRUENAS_URL= "https://192.168.2.7"
HOMEPAGE_VAR_TRUENAS_KEY="1-zM5Mm8l4VdRa4WNybTWjVdHVM89TR7fkGj40DkYEPgFdkt83DYBea95J3SHe3uCe"
HOMEPAGE_VAR_TRUENAS_ENABLEDPOOL= true
# Proxmox
HOMEPAGE_VAR_PROXMOX_ICON= "/icons/proxmox.png"
HOMEPAGE_VAR_PROXMOX_URL_PVE_1= "https://pve.d3adc3ii.site"
#HOMEPAGE_VAR_PROXMOX_USER= 
#HOMEPAGE_VAR_PROXMOX_API_KEY=
HOMEPAGE_VAR_PBS_URL= "https://pbs.d3adc3ii.site"

### INFRA ###
# Opnsense
HOMEPAGE_VAR_OPNSENSE_ICON= "/icons/opnsense.png"
HOMEPAGE_VAR_OPNSENSE_URL= "https://opn.d3adc3ii.site"
# DNS
HOMEPAGE_VAR_DNS_ICON= "/icons/technitium.png"
HOMEPAGE_VAR_DNS_URL= "https://ns0.d3adc3ii.site"
# MIKROTIK
HOMEPAGE_VAR_MIKROTIK_ICON= "/icons/mikrotik.png"
HOMEPAGE_VAR_MIKROTIK_URL= "http://192.168.2.253"
# TPLINK
HOMEPAGE_VAR_TPLINK_ICON= "/icons/tp-link.png"
HOMEPAGE_VAR_TPLINK_URL= "http://192.168.2.252"

### Monitoring ###
# Caddy Proxy
HOMEPAGE_VAR_CADDYPROXY_ICON= "/icons/caddy.png"
HOMEPAGE_VAR_CADDYPROXY_URL= "http://caddy:2020"
# NetAlertX 
HOMEPAGE_VAR_NETALERTX_ICON= "/icons/netalertx.png"
HOMEPAGE_VAR_NETALERTX_URL= "https://netalertx.d3adc3ii.site"
HOMEPAGE_VAR_NETALERTX_KEY="t_pJ26L2qEeUcR6bz5RCGI"
HOMEPAGE_VAR_NETALERTX_FIELDS= ["connected","down_alerts","new_devices"] 
# APPRISE-API
HOMEPAGE_VAR_APPRISE_ICON= "/icons/apprise.png"
HOMEPAGE_VAR_APPRISE_URL= "http://192.168.2.33:8000"
# Healthchecks
HOMEPAGE_VAR_HEALTHCHECKS_ICON= "/icons/healthchecks.png"
HOMEPAGE_VAR_HEALTHCHECKS_URL= "http://health.d3adc3ii.site"
# Smokeping
HOMEPAGE_VAR_SMOKEPING_ICON= "/icons/smokeping.png"
HOMEPAGE_VAR_SMOKEPING_URL= "http://192.168.2.21/smokeping"
# Guacamole
HOMEPAGE_VAR_GUACAMOLE_ICON= "/icons/guacamole.png"
HOMEPAGE_VAR_GUACAMOLE_URL= "https://guaca.int.d3adc3ii.cc/guacamole"
# Battle of the bandwidth
HOMEPAGE_VAR_BOTB_ICON= "/icons/smokeping.png"
HOMEPAGE_VAR_BOTB_URL= "https://botb.int.d3adc3ii.cc"

### BACKUP ###
# Battle of the bandwidth
HOMEPAGE_VAR_BACKREST_ICON= "/icons/backrest.png"
HOMEPAGE_VAR_BACKREST_URL= "https://backrest.d3adc3ii.site"


# Jellyfin
HOMEPAGE_VAR_JELLY_ICON= "/icons/jellyfin.png"
HOMEPAGE_VAR_JELLY_URL= "https://jelly.d3adc3ii.site"
HOMEAGE_VAR_JELLY_WIDGETURL= "http://192.168.2.33:8096"
HOMEPAGE_VAR_JELLY_KEY= "a6a45c61dd1d4f059b02fa22ad8c0ef3"
HOMEPAGE_VAR_JELLY_ENABLEBLOCK=true
HOMEPAGE_VAR_JELLY_NOWPLAY=false

# Immich
HOMEPAGE_VAR_IMMICH_ICON = "/icons/immich.png"
HOMEPAGE_VAR_IMMICH_URL= "https://immich.d3adc3ii.site"
HOMEPAGE_VAR_IMMICH_KEY= "7zmatjhj9RVRPylp08hKaG1mkYDubUyeqfDlpaUHo"
HOMEPAGE_VAR_IMMICH_VERSION= 2
HOMEPAGE_VAR_IMMICH_FIELDS= ["storage","photos","videos"] 

### LOCAL APPS ###
# Actual Budget
HOMEPAGE_VAR_ACTUAL_ICON= "/icons/actual-budget.png"
HOMEPAGE_VAR_ACTUAL_URL= "https://actual.d3adc3ii.site"
# Sensei
HOMEPAGE_VAR_SENSEI_ICON= "/icons/davis.png"
HOMEPAGE_VAR_SENSEI_URL= "https://statement.d3adc3ii.site"
# Homeassistant
HOMEPAGE_VAR_HA_ICON= "/icons/home-assistant.png"
HOMEPAGE_VAR_HA_URL= "https://ha.d3adc3ii.site"
# Qbit
HOMEPAGE_VAR_QBIT_ICON= "/icons/qbittorrent.png"
HOMEPAGE_VAR_QBIT_URL= "https://qbit.d3adc3ii.site"
HOMEPAGE_VAR_QBIT_USER= "d3adc3ii"
HOMEPAGE_VAR_QBIT_PASSWORD= "WCxb2CcykoUmjiNAf3p8"
# Stash
HOMEPAGE_VAR_STASH_ICON= "/icons/stash.png"
HOMEPAGE_VAR_STASH_URL= "https://stash.d3adc3ii.site"
HOMEPAGE_VAR_STASH_USER= "d3adc3ii"
HOMEPAGE_VAR_QBIT_API= "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ1aWQiOiJkMyIsInN1YiI6IkFQSUtleSIsImlhdCI6MTc0NjAzOTExOX0.OQqokQMNgdCRNxfPOQGOTFH4GCB0pQvZbIS9orkgkrg"
# Semaphore
HOMEPAGE_VAR_SEMAPHORE_ICON= "/icons/semaphore.png"
HOMEPAGE_VAR_SEMAPHORE_URL= "https://semaphore.d3adc3ii.site"
# Aria2
HOMEPAGE_VAR_ARIA_ICON= "/icons/ariang.png"
HOMEPAGE_VAR_ARIA_URL= "https://dl.d3adc3ii.site"

### EXTERNAL ###

# Pangolin
HOMEPAGE_VAR_PANGOLIN_ICON= "/icons/pangolin.png"
HOMEPAGE_VAR_PANGOLIN_URL= "https://pangolin.d3adc3ii.cc"
# Authentik
HOMEPAGE_VAR_AUTHENTIK_ICON= "/icons/authentik.png"
HOMEPAGE_VAR_AUTHENTIK_URL= "https://auth.d3adc3ii.cc"
HOMEPAGE_VAR_AUTHENTIK_KEY="Boy9GZM04gnN8zn1RdTDSxgkeDr70a8XqFKmrp1HwVx7A3u4GA2pnVN2aZEO"
# Komodo
HOMEPAGE_VAR_KOMODO_ICON= "/icons/komodo.png"
HOMEPAGE_VAR_KOMODO_URL= "https://komodo.d3adc3ii.cc"
# Twingate
HOMEPAGE_VAR_TWINGATE_ICON= "/icons/twingate.png"
HOMEPAGE_VAR_TWINGATE_URL= "https://d3net.twinget.com"
# Dumbterm
HOMEPAGE_VAR_DUMBTERM_ICON= "/icons/dumbterm.png"
HOMEPAGE_VAR_DUMBTERM_URL= "https://dumbterm.d3adc3ii.cc"

### Monitoring ###

# Grafana
HOMEPAGE_VAR_GRAFANA_ICON= "/icons/grafana.png"
HOMEPAGE_VAR_GRAFANA_URL= "https://d3adc3ii.grafana.net/dashboards"
# Beszel
HOMEPAGE_VAR_BESZEL_ICON= "/icons/beszel.png"
HOMEPAGE_VAR_BESZEL_URL= "https://beszel.d3adc3ii.cc"
HOMEPAGE_VAR_BESZEL_USERNAME= "d3social@pm.me"
HOMEPAGE_VAR_BESZEL_PASSWORD= "LWHAmofkUAagBb8Zx4onH3EiraK8ZCpauKZiGbr5Re"
HOMEPAGE_VAR_BESZEL_VERSION= "2"
# ZABBIX
HOMEPAGE_VAR_ZABBIX_ICON= "/icons/zabbix.png"
HOMEPAGE_VAR_ZABBIX_URL= "https://zabbix.int.d3adc3ii.cc/zabbix"
# Checkmk
HOMEPAGE_VAR_CHECKMK_ICON= "/icons/checkmk.png"
HOMEPAGE_VAR_CHECKMK_URL= "https://checkmk.d3adc3ii.cc/d3/check_mk"

### APPS ###
# Karakeep
HOMEPAGE_VAR_KARAKEEP_ICON= "/icons/karakeep.png"
HOMEPAGE_VAR_KARAKEEP_URL= "https://kara.d3adc3ii.cc"
# Wallos
HOMEPAGE_VAR_WALLOS_ICON= "/icons/wallos.png"
HOMEPAGE_VAR_WALLOS_URL= "https://wallos.d3adc3ii.cc"
# Ommni Tools
HOMEPAGE_VAR_OMNITOOLS_ICON= "/icons/omni-tools.png"
HOMEPAGE_VAR_OMNITOOLS_URL= "https://omnitools.d3adc3ii.cc"

### GL1 ###
# PVE11
HOMEPAGE_VAR_PVE11_URL= "https://pve11.d3adc3ii.cc"
HOMEPAGE_VAR_PVE12_URL= "https://pve12.d3adc3ii.cc"
HOMEPAGE_VAR_PVE13_URL= "https://pve13.d3adc3ii.cc"
HOMEPAGE_VAR_GLANCES_PVE11= "http://192.168.3.11:61208"
HOMEPAGE_VAR_GLANCES_PVE12= "http://192.168.3.12:61208"
HOMEPAGE_VAR_GLANCES_PVE13= "http://192.168.3.13:61208"
"""

##

[[stack]]
name = "immich"
tags = ["internal", "production"]
[stack.config]
server = "server-bkxal"
file_contents = """
networks:
  caddy:
    external: true
services:
  immich-server:
    container_name: immich_server
    image: ghcr.io/immich-app/immich-server:${IMMICH_VERSION:-release}
    networks:
      - caddy
    # extends:
    #   file: hwaccel.transcoding.yml
    #   service: cpu # set to one of [nvenc, quicksync, rkmpp, vaapi, vaapi-wsl] for accelerated transcoding
    volumes:
      # Do not edit the next line. If you want to change the media storage location on your system, edit the value of UPLOAD_LOCATION in the .env file
      - ${UPLOAD_LOCATION}:/usr/src/app/upload
      - /etc/localtime:/etc/localtime:ro
    env_file:
      - .env
    ports:
      - '2283:2283'
    depends_on:
      - redis
      - database
    restart: always
    healthcheck:
      disable: false
    labels:
      caddy: immich.d3adc3ii.site
      caddy.reverse_proxy: "{{upstreams 2283}}"

  immich-machine-learning:
    container_name: immich_machine_learning
    image: ghcr.io/immich-app/immich-machine-learning:${IMMICH_VERSION:-release}
    networks:
      - caddy
    volumes:
      - model-cache:/cache
    env_file:
      - .env
    restart: always
    healthcheck:
      disable: false

  redis:
    container_name: immich_redis
    image: docker.io/valkey/valkey:8-bookworm@sha256:42cba146593a5ea9a622002c1b7cba5da7be248650cbb64ecb9c6c33d29794b1
    networks:
      - caddy
    healthcheck:
      test: redis-cli ping || exit 1
    restart: always

  database:
    container_name: immich_postgres
    image: docker.io/tensorchord/pgvecto-rs:pg14-v0.2.0@sha256:739cdd626151ff1f796dc95a6591b55a714f341c737e27f045019ceabf8e8c52
    networks:
      - caddy
    environment:
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_USER: ${DB_USERNAME}
      POSTGRES_DB: ${DB_DATABASE_NAME}
      POSTGRES_INITDB_ARGS: '--data-checksums'
    volumes:
      # Do not edit the next line. If you want to change the database storage location on your system, edit the value of DB_DATA_LOCATION in the .env file
      - ${DB_DATA_LOCATION}:/var/lib/postgresql/data
    user: "1000:1000"
    healthcheck:
      test: >-
        pg_isready --dbname="$${POSTGRES_DB}" --username="$${POSTGRES_USER}" || exit 1; Chksum="$$(psql --dbname="$${POSTGRES_DB}" --username="$${POSTGRES_USER}" --tuples-only --no-align --command='SELECT COALESCE(SUM(checksum_failures), 0) FROM pg_stat_database')"; echo "checksum failure count is $$Chksum"; [ "$$Chksum" = '0' ] || exit 1
      interval: 5m
      start_interval: 30s
      start_period: 5m
    command: >-
      postgres -c shared_preload_libraries=vectors.so -c 'search_path="$$user", public, vectors' -c logging_collector=on -c max_wal_size=2GB -c shared_buffers=512MB -c wal_compression=on
    restart: always

volumes:
  model-cache:
"""
environment = """
UPLOAD_LOCATION=/mnt/zApps/immich-external
DB_DATA_LOCATION=/mnt/zApps/immich-external/postgres
IMMICH_VERSION=release
DB_PASSWORD=[[IMMICH_DB_PASSWORD]]
DB_USERNAME=postgres
DB_DATABASE_NAME=immich
"""

##

[[stack]]
name = "jellyfin"
tags = ["internal", "production"]
[stack.config]
server = "komodo-3"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
networks:
  caddy:
    external: true
services:
  jellyfin:
    image: lscr.io/linuxserver/jellyfin:latest
    container_name: jellyfin
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Asia/Singapore
      - JELLYFIN_PublishedServerUrl=https://jelly.int.d3adc3ii.cc #optional
    volumes:
      - ${JELLYFIN_CONFIG_DIR}:/config
      - ${JELLYFIN_DATA_DIR}/tvshows:/data/tvshows
      - ${JELLYFIN_DATA_DIR}/movies:/data/movies
    ports:
      - 8096:8096
      - 8920:8920 #optional
      - 7359:7359/udp #optional
      - 1900:1900/udp #optional
    restart: unless-stopped
    labels:
      caddy: jelly.d3adc3ii.site
      caddy.reverse_proxy: "{{upstreams 8096}}"
"""
environment = """
JELLYFIN_CONFIG_DIR=/mnt/zApps/jellyfin/config
JELLYFIN_DATA_DIR=/mnt/zFiles/media

"""

##

[[stack]]
name = "karakeep"
tags = ["external", "production"]
[stack.config]
server = "komodo-2"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
version: '3.8'
services:
  web:
    image: ghcr.io/karakeep-app/karakeep:${KARAKEEP_VERSION:-release}
    restart: unless-stopped
    volumes:
      #- /etc/komodo/repos/diiihl/app-data/karakeep-data:/data
      - ${DATA_DIR}:/data
    ports:
      - 3333:3000
    env_file:
      - .env
    environment:
      MEILI_ADDR: http://meilisearch:7700
      BROWSER_WEB_URL: http://chrome:9222
      DATA_DIR: /data
  chrome:
    image: gcr.io/zenika-hub/alpine-chrome:123
    restart: unless-stopped
    command:
      - --no-sandbox
      - --disable-gpu
      - --disable-dev-shm-usage
      - --remote-debugging-address=0.0.0.0
      - --remote-debugging-port=9222
      - --hide-scrollbars
  meilisearch:
    image: getmeili/meilisearch:v1.13.3
    restart: unless-stopped
    env_file:
      - .env
    environment:
      MEILI_NO_ANALYTICS: "true"
    volumes:
      - meilisearch:/meili_data

volumes:
  meilisearch:
  data:
"""
environment = """
KARAKEEP_VERSION=release
NEXTAUTH_SECRET=[[KARA_NEXTAUTH_SECRET]]
MEILI_MASTER_KEY=[[KARA_MEILI_MASTER_KEY]]
NEXTAUTH_URL=http://192.168.2.32:3333
OPENAI_API_KEY=[[OPENAI_API_KEY]]
DATA_DIR=/mnt/zApps/karakeep/data
"""

##

[[stack]]
name = "loggifly"
tags = ["internal", "monitoring"]
[stack.config]


##

[[stack]]
name = "mailrise"
[stack.config]
server = "komodo-3"
file_contents = """
services:
  mailrise:
    image: yoryan/mailrise:latest
    container_name: mailrise
    ports:
      - '8025:8025'
    restart: unless-stopped
    volumes:
      -  /mnt/zApps/mailrise/mailrise.conf:/etc/mailrise.conf:ro
"""
environment = """

"""

##

[[stack]]
name = "netalertx"
tags = ["internal", "monitoring"]
[stack.config]
server = "server-bkxal"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  netalertx:
    container_name: netalertx
    image: "ghcr.io/jokob-sk/netalertx:latest"      
    network_mode: "host"     
    restart: unless-stopped
    volumes:
      - ${DATA_DIR}/config:/app/config
      - ${DATA_DIR}/db:/app/db      
      - ${DATA_DIR}/logs:/app/log
      - type: tmpfs
        target: /app/api
    env_file:
      - .env
"""
environment = """
DATA_DIR=/mnt/zApps/netalertx
PUID=1000
PGID=1000
TZ=Asia/Singapore
PORT=20184
"""

##

[[stack]]
name = "ntfy"
tags = ["internal", "monitoring"]
[stack.config]
server = "komodo-2"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  ntfy:
    image: binwiederhier/ntfy
    container_name: ntfy
    command:
      - serve
    environment:
      - TZ=Asia/Singapore    # optional: set desired timezone
    user: 1000:1000 # optional: replace with your own user/group or uid/gid
    env_file:
      - .env
    volumes:
      - /var/cache/ntfy:/var/cache/ntfy
      - ${NTFY_DATA_DIR}:/etc/ntfy
    ports:
      - 8193:80
    healthcheck: # optional: remember to adapt the host:port to your environment
        test: ["CMD-SHELL", "wget -q --tries=1 http://localhost:80/v1/health -O - | grep -Eo '\"healthy\"\\s*:\\s*true' || exit 1"]
        interval: 60s
        timeout: 10s
        retries: 3
        start_period: 40s
    restart: unless-stopped
"""
environment = """
NTFY_DATA_DIR=/mnt/zApps/ntfy
"""

##

[[stack]]
name = "nzbget"
tags = ["internal", "testing"]
[stack.config]
server = "komodo-3"
poll_for_updates = true
auto_update = true
auto_update_all_services = true
file_contents = """
services:
  nzbget:
    image: lscr.io/linuxserver/nzbget:latest
    container_name: nzbget
    env_file:
      - .env
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
      - NZBGET_USER=nzbget #optional
      - NZBGET_PASS=tegbzn6789 #optional
    volumes:
      - ${NZBGET_CF_DIR}:/config
      - ${NZBGET_DL_DIR}:/downloads #optional
    ports:
      - 6789:6789
    restart: unless-stopped
"""
environment = """
NZBGET_CF_DIR=/mnt/zApps/nzb/config
NZBGET_DL_DIR=/mnt/zApps/nzb/downloads
"""

##

[[stack]]
name = "speedtest-tracker"
tags = ["internal", "testing"]
[stack.config]
server = "server-bkxal"
file_contents = """
services:
    speedtest-tracker:
        image: lscr.io/linuxserver/speedtest-tracker:latest
        restart: unless-stopped
        container_name: speedtest-tracker
        ports:
            - 8008:80
            - 8043:443
        environment:
            -APP_DEBUG=true
            - PUID=1000
            - PGID=1000
            - APP_KEY= ${APP_KEY}
            - DB_CONNECTION=${DB_CONNECTION}
          #  - DB_HOST=${DB_HOST}
          #  - DB_PORT=${DB_PORT}
           # - DB_DATABASE= ${DB_DATABASE}
           # - DB_USERNAME= ${DB_USERNAME}
           # - DB_PASSWORD= ${DB_PASSWORD}
        volumes:
            - ${DATA_DIR}/data:/config
            #- /path/to-custom-ssl-keys:/config/keys
"""
environment = """
DATA_DIR="/mnt/zApps/speedtest-tracker"
APP_KEY="Kj1qpSM25ly7LbReMibYBly5Icl9D2TQXTHi8Xo+bIA="
DB_CONNECTION=sqllite
DB_HOST= db
DB_PORT=3306
DB_DATABASE=speedtest_tracker
DB_USERNAME=d3
DB_PASSWORD="ng--*UC*6W2Fz89GhAR_Bm9ZC3cx2okJ4yCTEhieCYX!MYL6C_VDaYcvmnxffVj*9_H.t2x@.@UC4hKx8nUrxJjrTb_NZqB4bYea"
"""

##

[[deployment]]
name = "apprise"
[deployment.config]
server = "komodo-3"
image.type = "Image"
image.params.image = "caronc/apprise:latest"
poll_for_updates = true
auto_update = true
network = "bridge"
ports = """
8000:8000
"""
volumes = """
/mnt/zApps/apprise_api/config:/opt/apprise/config
/mnt/zApps/apprise_api/plugin:/opt/apprise/plugin
/mnt/zApps/apprise_api/attach:/opt/apprise/attach
"""
environment = """
PUID=1000
PGID=1000
APPRISE_STATEFUL_MODE=simple
APPRISE_WORKER_COUNT=1
"""

##

[[deployment]]
name = "beszel-k11"
tags = ["internal", "monitoring"]
[deployment.config]
server = "server-bkxal"
image.type = "Image"
image.params.image = "henrygd/beszel-agent"
poll_for_updates = true
auto_update = true
restart = "unless-stopped"
volumes = """
/var/run/docker.sock:/var/run/docker.sock:ro
"""
environment = """
LISTEN=45876
KEY="ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJq8OcGRupFySjJHa5aIjeWFHd17+Lfwn+TVfwU6cTY/"
  # VARIABLE = value
"""

##

[[deployment]]
name = "beszel-k12"
tags = ["internal", "monitoring"]
[deployment.config]
server = "komodo-2"
image.type = "Image"
image.params.image = "henrygd/beszel-agent"
poll_for_updates = true
auto_update = true
restart = "unless-stopped"
volumes = """
/var/run/docker.sock:/var/run/docker.sock:ro
"""
environment = """
LISTEN=45876
KEY="ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIJq8OcGRupFySjJHa5aIjeWFHd17+Lfwn+TVfwU6cTY/"
"""

##

[[deployment]]
name = "nessus"
tags = ["internal", "testing"]
[deployment.config]
server = "komodo-2"
image.type = "Image"
image.params.image = "tenable/nessus:latest-ubuntu"

##

[[deployment]]
name = "newt1"
tags = ["external", "production"]
[deployment.config]
server = "server-bkxal"
image.type = "Image"
image.params.image = "fosrl/newt"
poll_for_updates = true
auto_update = true
restart = "unless-stopped"
environment = """
  PANGOLIN_ENDPOINT=https://pangolin.d3adc3ii.cc
  NEWT_ID=[[NEWT_ID1]]
  NEWT_SECRET=[[NEWT_SECRET1]]
"""

##

[[deployment]]
name = "omni-tools"
tags = ["external", "production"]
[deployment.config]
server = "komodo-2"
image.type = "Image"
image.params.image = "iib0011/omni-tools:latest"
poll_for_updates = true
auto_update = true
network = "bridge"
restart = "unless-stopped"
ports = """
8281:80
"""

##

[[deployment]]
name = "statementsensei"
tags = ["internal", "testing"]
[deployment.config]
server = "komodo-2"
image.type = "Image"
image.params.image = "benjaminawd/statementsensei:latest"
poll_for_updates = true
auto_update = true
network = "--network caddy external: true"
ports = """
8501:8501
"""
environment = """
  PDF_PASSWORD= [[PDF_PASSWORD]]
"""
labels = """
caddy: statement.d3adc3ii.site
caddy.reverse_proxy: "{{upstreams 8501}}"
"""

##

[[deployment]]
name = "true-command"
tags = ["internal", "testing"]
[deployment.config]
server = "komodo-2"
image.type = "Image"
image.params.image = "ixsystems/truecommand"
poll_for_updates = true
auto_update = true
network = "bridge"
restart = "unless-stopped"
ports = """
8880:80
4443:443
"""
volumes = """
/mnt/zApps/truecommand:/data
"""

##

[[deployment]]
name = "twingate"
tags = ["external", "production"]
[deployment.config]
server = "komodo-2"
image.type = "Image"
image.params.image = "twingate/connector:latest"
poll_for_updates = true
auto_update = true
restart = "unless-stopped"
environment = """
  TWINGATE_NETWORK=d3net
  TWINGATE_ACCESS_TOKEN=eyJhbGciOiJFUzI1NiIsImtpZCI6Inp3dkU1dHpJZzV4X2pSVEU4RTFWQll6MW0tX2g1dXlMZlhTV1VSS1BEVE0iLCJ0eXAiOiJEQVQifQ.eyJudCI6IkFOIiwiYWlkIjoiNTExMjE5IiwiZGlkIjoiMjE5MTEwMCIsImp0aSI6ImIxZjU3N2FkLTZmNDItNDYyYS05ZGIzLTY1NTE5ZmQyMTJlNCIsImlzcyI6InR3aW5nYXRlIiwiYXVkIjoiZDNuZXQiLCJleHAiOjE3NDYxNDQ5MDAsImlhdCI6MTc0NjE0MTMwMCwidmVyIjoiNCIsInRpZCI6IjEwMzU2NCIsInJudyI6MTc0NjE0MTU3OCwicm5ldGlkIjoiMTM1OTMxIn0.OxT4qXnqonLPGb1GwJTRcYoSwZG16x2JGA_Xu2pOo0dZH3jpqfd1SkjWy8JjcVePboTum2e0WEdNu4SFcJUy_A
  TWINGATE_REFRESH_TOKEN=mrNJNc7hirY3gO3-q9l6dnN_YjYl-8q79XRXg1ffZstYm8EYyH6xNXMIVviMaQ-2-GAa4wuSMv1J5ebEf9KvzIK94jmq7j9QGPH2Tr7ZnjlYADuKrEpKUkrmnbrROkgQy6nzWg
  TWINGATE_LOG_ANALYTICS=v2
"""

##

[[repo]]
name = "diiihl"
[repo.config]
server = "server-bkxal"
builder = "local"
git_account = "d3adc3iii"
repo = "d3adc3iii/dIIIhl"

##

[[repo]]
name = "diiihl-k2"
[repo.config]
server = "komodo-2"
builder = "local"
git_account = "d3adc3iii"
repo = "d3adc3iii/dIIIhl"

##

[[repo]]
name = "diiihl-k3"
[repo.config]
server = "komodo-3"
builder = "local"
git_account = "d3adc3iii"
repo = "d3adc3iii/dIIIhl"

##

[[procedure]]
name = "autosync"
config.schedule = "Run every 30 minutes"

[[procedure.config.stage]]
name = "Stage 1"
enabled = true
executions = [
  { execution.type = "RunSync", execution.params.sync = "sync", enabled = true },
  { execution.type = "CommitSync", execution.params.sync = "sync", enabled = true }
]

##

[[procedure]]
name = "check_update"

##

[[procedure]]
name = "pull-deploy"
description = "description = \"Pulls stack-repo, deploys stacks\""
config.schedule_timezone = "Asia/Singapore"

[[procedure.config.stage]]
name = "\"Pull Repo"
enabled = true
executions = [
  { execution.type = "PullRepo", execution.params.repo = "diiihl", enabled = true }
]

[[procedure.config.stage]]
name = "Deploy if changed"
enabled = true
executions = [
  { execution.type = "BatchDeployStackIfChanged", execution.params.pattern = "t*,o*,a*", enabled = true }
]

##

[[builder]]
name = "local"
[builder.config]
type = "Server"
params.server_id = "server-bkxal"

##

[[resource_sync]]
name = "sync"
[resource_sync.config]
repo = "d3adc3iii/dIIIhl"
git_account = "d3adc3iii"
resource_path = ["komodo/resources/main.toml"]
managed = true
delete = true
include_user_groups = true